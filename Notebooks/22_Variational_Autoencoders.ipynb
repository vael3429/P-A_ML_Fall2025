{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import umap.umap_ as umap\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f1a3c",
   "metadata": {},
   "source": [
    "Import MNIST dataset and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: normalize and convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Download training and test data\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=False, transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed4c0d",
   "metadata": {},
   "source": [
    "Let's take a look at our training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd56ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Training set size: {len(train_subset)} images\")\n",
    "print(f\"Validation set size: {len(val_subset)} images\")\n",
    "print(f\"Test set size:     {len(test_dataset)} images\")\n",
    "\n",
    "# Look at a single sample\n",
    "img, label = train_subset[0]\n",
    "print(f\"\\nSample image shape: {img.shape}\")\n",
    "print(f\"Sample label: {label}\")\n",
    "\n",
    "# Display a grid of samples\n",
    "def show_batch(dataloader, n=32):\n",
    "    imgs, labels = next(iter(dataloader))\n",
    "    grid = make_grid(imgs[:n], nrow=8, padding=2)\n",
    "    npimg = grid.numpy()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\"gray\")\n",
    "    plt.title(\"Batch of MNIST digits\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c625b4",
   "metadata": {},
   "source": [
    "Here we define a new model, based on a Variational Autoencodder architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e14762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_VAE_2D(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),   # -> (16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # -> (32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(32*7*7, 256)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(latent_dim, 256)\n",
    "        self.fc_dec = nn.Linear(256, 32*7*7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # -> (16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),   # -> (1, 28, 28)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.fc2(z)\n",
    "        x = self.fc_dec(x)\n",
    "        x = x.view(-1, 32, 7, 7)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "    \n",
    "    def vae_loss(self, recon_x, x, mu, logvar, beta=1):\n",
    "        # Reconstruction (sum and normalize by batch)\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='sum') / x.size(0)\n",
    "        # KL divergence (analytical)\n",
    "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl /= x.size(0)  # normalize per batch\n",
    "\n",
    "        return recon_loss + beta * kl, recon_loss, kl\n",
    "\n",
    "conv_vae_2d = Conv_VAE_2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144dd89",
   "metadata": {},
   "source": [
    "This function is similar to the one we defined for the standard Autoencoder, with some modifications to account for the variational structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_batch(x, y):\n",
    "    \"\"\"Compute mean cosine similarity between two image batches.\"\"\"\n",
    "    x = x.view(x.size(0), -1)\n",
    "    y = y.view(y.size(0), -1)\n",
    "    sim = F.cosine_similarity(x, y, dim=1)\n",
    "    return sim.mean().item()\n",
    "\n",
    "def train_vae(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    beta=1.0,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_cos_history = []\n",
    "    val_loss_history = []\n",
    "    val_cos_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_cos = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for imgs, _ in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            recon, mu, logvar = model(imgs)\n",
    "            loss, recon_loss, kl_loss = model.vae_loss(recon, imgs, mu, logvar, beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # compute cosine on detached tensors to avoid extra grad graph\n",
    "            running_cos += cosine_similarity_batch(recon.detach(), imgs.detach())\n",
    "            n_batches += 1\n",
    "\n",
    "        epoch_train_loss = running_loss / n_batches\n",
    "        epoch_train_cos = running_cos / n_batches\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        train_cos_history.append(epoch_train_cos)\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_cos = 0.0\n",
    "        n_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                recon, mu, logvar = model(imgs)\n",
    "                loss, _, _ = model.vae_loss(recon, imgs, mu, logvar, beta)\n",
    "\n",
    "                running_val_loss += loss.item()\n",
    "                running_val_cos += cosine_similarity_batch(recon, imgs)\n",
    "                n_val_batches += 1\n",
    "\n",
    "        epoch_val_loss = running_val_loss / n_val_batches\n",
    "        epoch_val_cos = running_val_cos / n_val_batches\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        val_cos_history.append(epoch_val_cos)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{epochs}: \"\n",
    "            f\"TrainLoss={epoch_train_loss:.4f}, TrainCos={epoch_train_cos:.4f} | \"\n",
    "            f\"ValLoss={epoch_val_loss:.4f}, ValCos={epoch_val_cos:.4f}\"\n",
    "        )\n",
    "\n",
    "    # ---- Plot results ----\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\", color=\"tab:blue\")\n",
    "    ax1.plot(epochs_range, train_loss_history, label=\"Train Loss\", color=\"tab:blue\")\n",
    "    ax1.plot(epochs_range, val_loss_history,   label=\"Val Loss\",   color=\"tab:cyan\")\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"Cosine Similarity\", color=\"tab:red\")\n",
    "    ax2.plot(epochs_range, train_cos_history, label=\"Train CosSim\", color=\"tab:red\", linestyle='--')\n",
    "    ax2.plot(epochs_range, val_cos_history,   label=\"Val CosSim\",   color=\"tab:pink\", linestyle='--')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    # Put a combined legend\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + lines2, labels + labels2, loc=\"upper right\")\n",
    "\n",
    "    plt.title(\"VAE: Loss & Cosine Similarity per Epoch\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918bef16",
   "metadata": {},
   "source": [
    "Let's train the variational autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vae(conv_vae_2d, train_loader, val_loader, beta = 0.1, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e255ca",
   "metadata": {},
   "source": [
    "Let's visualize the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ca354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions_vae(model, dataloader, n=10):\n",
    "    model.eval()\n",
    "    imgs, _ = next(iter(dataloader))\n",
    "    imgs = imgs[:n]\n",
    "    with torch.no_grad():\n",
    "        recon,_,_ = model(imgs)\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(imgs[i].squeeze(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        # Reconstructed\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(recon[i].squeeze(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstructions_vae(conv_vae_2d, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426e72e",
   "metadata": {},
   "source": [
    "Here we plot the final latent space distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space_vae(model, dataloader, n_batches=10):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, lbls) in enumerate(dataloader):\n",
    "            \n",
    "            # Pass only through the encoder\n",
    "            z, _ = model.encode(imgs)\n",
    "            latents.append(z.cpu())\n",
    "            labels.append(lbls)\n",
    "            \n",
    "            if i >= n_batches:  # limit batches for speed\n",
    "                break\n",
    "\n",
    "    latents = torch.cat(latents)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    if len(latents[0]) == 2:\n",
    "        additional = ''\n",
    "        latents_ = latents\n",
    "    else:\n",
    "        reducer = umap.UMAP(n_neighbors=20, min_dist=0.05, n_components=2,\n",
    "                    metric='euclidean', random_state=42)\n",
    "        print(latents.shape)\n",
    "        latents_ = reducer.fit_transform(latents)\n",
    "        additional = ', with UMAP reduction'\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(latents_[:, 0], latents_[:, 1], c=labels, cmap=\"tab10\", s=10, alpha=0.7)\n",
    "    plt.colorbar(scatter, ticks=range(10))\n",
    "    plt.title(f\"2D Latent Space (MNIST) {additional}\")\n",
    "    plt.xlabel(\"z₁\")\n",
    "    plt.ylabel(\"z₂\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_latent_space_vae(conv_vae_2d, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_posterior_estimate(model, dataloader, n_samples=5):\n",
    "    model.eval()\n",
    "\n",
    "    imgs, _ = next(iter(dataloader))\n",
    "    with torch.no_grad():\n",
    "        mu, logvar = model.encode(imgs)\n",
    "    \n",
    "    # Mean of posterior approx\n",
    "    mean = mu.mean(dim=0)\n",
    "    std = mu.std(dim=0)\n",
    "\n",
    "    z = torch.randn(n_samples, model.latent_dim) * std + mean\n",
    "\n",
    "    with torch.no_grad():\n",
    "        decoded = model.decode(z)\n",
    "\n",
    "    plt.figure(figsize=(n_samples * 2, 2))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(1, n_samples, i+1)\n",
    "        plt.imshow(decoded[i].squeeze().cpu(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(\"Samples near posterior latent distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "sample_from_posterior_estimate(conv_vae_2d, test_loader, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e381e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
