{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3affdb",
   "metadata": {},
   "source": [
    "# Linear Neural Network Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f1934",
   "metadata": {},
   "source": [
    "## Image Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa625e3",
   "metadata": {},
   "source": [
    "In the following sections, our discussion will center on the Fashion-MNIST dataset (Xiao et al., 2017) where consists of images representing 10 categories of clothing at the same pixel resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.use_svg_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325ebf0",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FashionMNIST(d2l.DataModule):  # @save\n",
    "    \"\"\"The Fashion-MNIST dataset.\"\"\"\n",
    "    def __init__(self, batch_size=64, resize=(28, 28), root=\"./data\", download=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.root = root   # ✅ override default \"../data\"\n",
    "\n",
    "        trans = transforms.Compose([\n",
    "            transforms.Resize(resize),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.train = torchvision.datasets.FashionMNIST(\n",
    "            root=self.root, train=True, transform=trans, download=download\n",
    "        )\n",
    "        self.val = torchvision.datasets.FashionMNIST(\n",
    "            root=self.root, train=False, transform=trans, download=download\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d2a32a",
   "metadata": {},
   "source": [
    "Fashion-MNIST consists of images from 10 categories, each represented by 6000 images in the training dataset and by 1000 in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FashionMNIST(resize=(28, 28), root=\"/tmp/fashion_mnist\", download=False)\n",
    "len(data.train), len(data.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5e6f9",
   "metadata": {},
   "source": [
    "The images are grayscale and upscaled to  $32 \\times 32$ pixels in resolution above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train[2][1]#.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e58f74",
   "metadata": {},
   "source": [
    "The Fashion-MNIST categories are labeled with names that are easily understood by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd842d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(FashionMNIST)  #@save\n",
    "def text_labels(self, indices):\n",
    "    \"\"\"Return text labels.\"\"\"\n",
    "    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "              'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [labels[int(i)] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150fee0",
   "metadata": {},
   "source": [
    "### Minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48244c0",
   "metadata": {},
   "source": [
    "For convenience, we rely on the built-in data iterator to read from the training and test sets instead of implementing one manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a99d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(FashionMNIST)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    data = self.train if train else self.val\n",
    "    return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\n",
    "                                       num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59048095",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print(X.shape, X.dtype, y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14c720",
   "metadata": {},
   "source": [
    "To illustrate this, we load a minibatch of images using the `train_dataloader` method, which provides 64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print(X.shape, X.dtype, y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ba63a",
   "metadata": {},
   "source": [
    "Let’s look at the time it takes to read the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "for X, y in data.train_dataloader():\n",
    "    continue\n",
    "f'{time.time() - tic:.2f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eb657",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a22b39",
   "metadata": {},
   "source": [
    "The helper function `show_images` allows us to display the images along with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2461f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(FashionMNIST)  #@save\n",
    "def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n",
    "    X, y = batch\n",
    "    if not labels:\n",
    "        labels = self.text_labels(y)\n",
    "    d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)\n",
    "batch = next(iter(data.val_dataloader()))\n",
    "data.visualize(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb0742",
   "metadata": {},
   "source": [
    "## The Base Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9b807",
   "metadata": {},
   "source": [
    "This section introduces a base class for classification models, designed to streamline and simplify future implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b3411",
   "metadata": {},
   "source": [
    "### The Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74778123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(d2l.Module):  #@save\n",
    "    \"\"\"The base class of classification models.\"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f472d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Module)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159942c",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6aff81",
   "metadata": {},
   "source": [
    "In classification tasks, we select the class with the highest predicted probability as the output, and accuracy is defined as the fraction of predictions that match the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(Classifier)  #@save\n",
    "def accuracy(self, Y_hat, Y, averaged=True):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
    "    preds = Y_hat.argmax(axis=1).type(Y.dtype)\n",
    "    compare = (preds == Y.reshape(-1)).type(torch.float32)\n",
    "    return compare.mean() if averaged else compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9ee03",
   "metadata": {},
   "source": [
    "## Softmax Regression, Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e5809",
   "metadata": {},
   "source": [
    "### The Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d22e4f",
   "metadata": {},
   "source": [
    "For a matrix $X$, we can sum either all elements by default or restrict the summation to a specific axis. The `axis` parameter allows us to calculate sums across rows or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "X.sum(0, keepdims=True), X.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c597b",
   "metadata": {},
   "source": [
    "The softmax computation involves three steps: (i) exponentiating each element, (ii) summing the values in each row to obtain the normalization constant for that example, and (iii) dividing each row’s elements by its normalization constant so that the outputs sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc81e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdims=True)\n",
    "    return X_exp / partition  # The broadcasting mechanism is applied here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95d0ea",
   "metadata": {},
   "source": [
    "For any input $X$, each element is transformed into a nonnegative value. Each row is then normalized so that its entries sum to 1, satisfying the requirements of a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((2, 5))\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a5780",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b345d86",
   "metadata": {},
   "source": [
    "we now implement the softmax regression model. In softmax regression, the network’s output size must match the number of classes. Therefore, with a dataset containing 10 classes, the model’s output dimension is set to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f555eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegressionScratch(d2l.Classifier):\n",
    "    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W = torch.normal(0, sigma, size=(num_inputs, num_outputs),\n",
    "                              requires_grad=True)\n",
    "        self.b = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94995f3",
   "metadata": {},
   "source": [
    "The code below defines how the network maps each input to an output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba732d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(SoftmaxRegressionScratch)\n",
    "def forward(self, X):\n",
    "    X = X.reshape((-1, self.W.shape[0]))\n",
    "    return softmax(torch.matmul(X, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8822b46",
   "metadata": {},
   "source": [
    "### The Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba170d1",
   "metadata": {},
   "source": [
    "Next we need to implement the cross-entropy loss function.\n",
    "\n",
    "Recall that cross-entropy takes the negative log-likelihood of the predicted probability as- signed to the true label. For efficiency we avoid Python for-loops and use indexing instead. In particular, the one-hot encoding in y allows us to select the matching terms in y_hat.\n",
    "\n",
    "To see this in action we create sample data y_hat with 2 examples of predicted probabilities over 3 classes and their corresponding labels y. The correct labels are 0 and 2 respectively (i.e., the first and third class). Using y as the indices of the probabilities in y_hat, we can pick out terms efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4095d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3598fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y, test=False):\n",
    "    if test:\n",
    "        print('print the highest probabilities', y_hat[list(range(len(y_hat))), y])\n",
    "        print('and their logarithm', torch.log(y_hat[list(range(len(y_hat))), y]))\n",
    "    return -torch.log(y_hat[list(range(len(y_hat))), y]).mean()\n",
    "\n",
    "cross_entropy(y_hat, y, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(SoftmaxRegressionScratch)\n",
    "def loss(self, y_hat, y):\n",
    "    return cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46665f0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67d040",
   "metadata": {},
   "source": [
    "We reuse the fit method defined in previous sections to train the model with 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "model = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca12e5",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2cc4f",
   "metadata": {},
   "source": [
    "With training finished, the model is now prepared to perform image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.val_dataloader()))\n",
    "preds = model(X).argmax(axis=1)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35753658",
   "metadata": {},
   "source": [
    "Our main focus is on the images that were misclassified. We display them by showing their true labels (top line of the text output) alongside the model’s predicted labels (bottom line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = preds.type(y.dtype) != y\n",
    "X, y, preds = X[wrong], y[wrong], preds[wrong]\n",
    "labels = [a+'\\n'+b for a, b in zip(\n",
    "    data.text_labels(y), data.text_labels(preds))]\n",
    "data.visualize([X, y], labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c766b5a",
   "metadata": {},
   "source": [
    "## Softmax Regression, Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3693f",
   "metadata": {},
   "source": [
    "Similar to how high-level deep learning frameworks simplified the implementation of linear regression, they prove equally convenient in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a2067",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(d2l.Classifier):  #@save\n",
    "    \"\"\"The softmax regression model.\"\"\"\n",
    "    def __init__(self, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(),\n",
    "                                 nn.LazyLinear(num_outputs))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abacf9",
   "metadata": {},
   "source": [
    "### Softmax Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d26fac",
   "metadata": {},
   "source": [
    "Directly computing softmax followed by cross-entropy can cause numerical instability due to overflow or underflow in the exponentiation step. To address this, modern implementations combine softmax and cross-entropy into a single operation (using techniques like the LogSumExp trick), which ensures stability while still allowing us to compute probabilities when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Classifier)  #@save\n",
    "def loss(self, Y_hat, Y, averaged=True):\n",
    "    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
    "    Y = Y.reshape((-1,))\n",
    "    return F.cross_entropy(\n",
    "        Y_hat, Y, reduction='mean' if averaged else 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c4611",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "model = SoftmaxRegression(num_outputs=10, lr=0.1)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30a7c3",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72f80c",
   "metadata": {},
   "source": [
    "1. Does reducing the batch_size (for instance, to 1) affect the reading performance?\n",
    "\n",
    "2. Deep learning uses many different number formats, including FP64 double precision (used extremely rarely), FP32 single precision, BFLOAT16 (good for compressed representations), FP16 (very unstable), TF32 (a new format from NVIDIA), and INT8. Compute the smallest and largest argument of the exponential function for which the result does not lead to numerical underflow or overflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
