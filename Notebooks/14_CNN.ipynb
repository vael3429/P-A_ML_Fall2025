{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d88c4d",
   "metadata": {},
   "source": [
    "# Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ca9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c8679",
   "metadata": {},
   "source": [
    "## You start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a9a9e",
   "metadata": {},
   "source": [
    "Write your own convolutional operation based on what expleained in class. Here some reminders:\n",
    "- X is the input matrix\n",
    "- K is the kernel (filter)\n",
    "- Y is the output matrix \n",
    "\n",
    "Suggested Steps:\n",
    "- Define an empty matrix Y with the correct shape\n",
    "- fill in the matrix, element-by-element, with the result of the convolution operation between X and K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    Y = torch.empty((X.shape[0] - K.shape[0] + 1, X.shape[1] - K.shape[1] + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + K.shape[0], j:j + K.shape[1]] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed506c4",
   "metadata": {},
   "source": [
    "The next steps will make use of this function you created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b66b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a generic X tensor and a kernel K\n",
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19edb2c",
   "metadata": {},
   "source": [
    "If you wrote the function correctly, you should get the following  Y tensor\n",
    "\n",
    "`tesor([[19, 25],[37, 43]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b74eb6",
   "metadata": {},
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21428b6c",
   "metadata": {},
   "source": [
    "A convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an output. \n",
    "\n",
    "The two parameters of a convolutional layer are the kernel and the scalar bias. We typically initialize the kernels randomly, just as we would with a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53359f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd3f2d",
   "metadata": {},
   "source": [
    "### Object Edge Detection in Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404da0c",
   "metadata": {},
   "source": [
    "Let‚Äôs take a moment to parse a simple application of a convolutional layer: detecting the edge of an object in an image by finding the location of the pixel change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78764b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, contruct an image X: middle four columns are black (0) and the rest are white (1)\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "print(X)\n",
    "\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d00ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we construct a kernel K with a height of 1 and a width of 2. \n",
    "# When we perform the cross-correlation operation with the input, \n",
    "# if the horizontally adjacent elements are the same, the output is 0. \n",
    "# Otherwise, the output is nonzero.\n",
    "K = torch.tensor([[1, -1]])\n",
    "print(K)\n",
    "\n",
    "plt.imshow(K, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c33dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the kernel K to detect the vertical edges of the object in the input X.\n",
    "Y = corr2d(X, K)\n",
    "print(Y)\n",
    "\n",
    "plt.imshow(Y, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now apply the kernel to the transposed image. \n",
    "# As expected, it vanishes. The kernel K only detects vertical edges.\n",
    "corr2d(X.t(), K)\n",
    "\n",
    "plt.imshow(corr2d(X.t(), K), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fb770",
   "metadata": {},
   "source": [
    "## Learning a Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93b47a",
   "metadata": {},
   "source": [
    "Designing an edge detector by finite differences [1, -1] is neat if we know this is precisely what we are looking for. However, as we look at larger kernels, and consider successive layers of convolutions, it might be impossible to specify precisely what each filter should be doing manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507392be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and\n",
    "# output in the format of (example, channel, height, width), where the batch\n",
    "# size (number of examples in the batch) and the number of channels are both 1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2  # Learning rate\n",
    "\n",
    "for i in range(20):\n",
    "    # make the prediction\n",
    "    Y_hat = conv2d(X)\n",
    "    # compute the loss\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    # compute the gradient\n",
    "    conv2d.zero_grad() # Zero the gradients before running the backward pass.\n",
    "    l.sum().backward()\n",
    "    # Update the kernel\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i + 1}, loss {l.sum():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d489560",
   "metadata": {},
   "source": [
    "Note that the error has dropped to a small value after 10 iterations. Now we will take a look at the kernel tensor we learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36557f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa4cd0",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb01f96",
   "metadata": {},
   "source": [
    "### Es 1\n",
    "\n",
    "What if such layers perform strict convolution operations (in the mathematical sense) instead of cross-correlations? \n",
    "\n",
    "How do you need to change the kernel (or the input tensor) in order to test this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06456af5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47580b5e",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b671eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a helper function to calculate convolutions. It initializes the\n",
    "# convolutional layer weights and performs corresponding dimensionality\n",
    "# elevations and reductions on the input and output\n",
    "\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # (1, 1) indicates that batch size and the number of channels are both 1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # Strip the first two dimensions: examples and channels\n",
    "    return Y.reshape(Y.shape[2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ada5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 row and column is padded on either side, so a total of 2 rows or columnsare added\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3a948",
   "metadata": {},
   "source": [
    "When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different padding numbers for height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a convolution kernel with height 5 and width 3. The padding on either\n",
    "# side of the height and width are 2 and 1, respectively\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128179b1",
   "metadata": {},
   "source": [
    "# Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ee6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let‚Äôs look at a slightly more complicated example.\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81959d",
   "metadata": {},
   "source": [
    "# Multiple Input Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdaedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    # Iterate through the 0th dimension (channel) of K first, then add them up\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06feaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "corr2d_multi_in(X, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676a8c9",
   "metadata": {},
   "source": [
    "# Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ff2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # Iterate through the 0th dimension of K, and each time, perform\n",
    "    # cross-correlation operations with input X. All of the results are\n",
    "    # stacked together\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n",
    "\n",
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ab020",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr2d_multi_in_out(X, K)\n",
    "\n",
    "# this creates 3 feature maps in output!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30425f43",
   "metadata": {},
   "source": [
    "# 1x1 Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde11785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    # Matrix multiplication in the fully connected layer\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d385d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d12efb",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa17edc5",
   "metadata": {},
   "source": [
    "Returning to the problem of edge detection, we use the output of the convolutional layer as input for 2 √ó 2 max-pooling. Denote by X the input of the convolutional layer input and Y the pooling layer output. Regardless of whether or not the values of X[i, j], X[i, j + 1], X[i+1, j] and X[i+1, j + 1] are different, the pooling layer always outputs Y[i, j] = 1. That is to say, using the 2 √ó 2 max-pooling layer, we can still detect if the pattern recognized by the convolutional layer moves no more than one element in height or width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab748722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    \n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d(X, (2, 2), 'avg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac88ed",
   "metadata": {},
   "source": [
    "We can demonstrate the use of padding and strides in pooling layers via the built-in two-dimensional max-pooling layer from the deep learning framework.\n",
    "\n",
    "NOTE: Since pooling aggregates information from an area, deep learning frameworks default to matching pooling window sizes and stride. For instance, if we use a pooling window of shape (3, 3) we get a stride shape of (3, 3) by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d(3)\n",
    "# by default stride = pool size\n",
    "# Pooling has no model parameters, hence it needs no initialization\n",
    "pool2d(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
    "pool2d(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb13310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what happens for multiple channels\n",
    "\n",
    "X = torch.cat((X, X + 1), 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5becc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c93f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50ba173d",
   "metadata": {},
   "source": [
    "# Transpose Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9bf79b",
   "metadata": {},
   "source": [
    "Ignoring channels for now, let‚Äôs begin with the basic transposed convolution operation with\n",
    "stride of 1 and no padding. \n",
    "\n",
    "Suppose that we are given a $ùëõ_‚Ñé \\times ùëõ_ùë§$ input tensor and a $ùëò_‚Ñé \\times ùëò_ùë§$\n",
    "kernel. \n",
    "\n",
    "Procedure:\n",
    "*  Sliding the kernel window with stride of 1 --> we get  $ùëõ_‚Ñé \\times ùëõ_ùë§$ intermediate results\n",
    "*  Each intermediate result is a ($ùëõ_‚Ñé + ùëò_‚Ñé ‚àí 1) \\times (ùëõ_ùë§ + ùëò_ùë§ ‚àí 1)$ tensor that are initialized as zeros\n",
    "*  Each element in the input tensor is multiplied by the kernel so that the resulting $ùëò_‚Ñé \\times ùëò_ùë§$ tensor replaces a portion in each intermediate tensor.\n",
    "*  All intermetiate results are summed over to produce the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_conv(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Y[i: i + h, j: j + w] += X[i, j] * K\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "trans_conv(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291d9c7",
   "metadata": {},
   "source": [
    "Alternatively, when the input Xand kernel Kare both fourth-order tensors, we can use high-level APIs to obtain the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728281c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e26594",
   "metadata": {},
   "source": [
    "## Padding, Strides, and Multiple Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b94a862",
   "metadata": {},
   "source": [
    "Different from in the regular convolution where padding is applied to input, it is applied to\n",
    "output in the transposed convolution. \n",
    "\n",
    "For example, when specifying the padding number\n",
    "on either side of the height and width as 1, the first and last rows and columns will be\n",
    "__removed__ from the transposed convolution output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056225c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fcc816",
   "metadata": {},
   "source": [
    "In the transposed convolution, strides are specified for intermediate results (thus output),\n",
    "not for input.\n",
    "\n",
    "Changing the stride from 1 to 2 increases both the height and width of intermediate tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6596f",
   "metadata": {},
   "source": [
    "For multiple input and output channels, the transposed convolution works in the same way\n",
    "as the regular convolution.\n",
    "\n",
    "When multiple output\n",
    "channels are specified, we will have a ùëêùëñ √ó ùëò‚Ñé √ó ùëòùë§ kernel for each output channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df165e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(size=(1, 10, 16, 16))\n",
    "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\n",
    "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\n",
    "tconv(conv(X)).shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6e91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
