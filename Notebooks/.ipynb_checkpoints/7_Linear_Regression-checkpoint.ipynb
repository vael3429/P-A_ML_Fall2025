{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa6e2b1",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a546d4",
   "metadata": {},
   "source": [
    "## From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b630b",
   "metadata": {},
   "source": [
    "This section walks through building a complete linear regression implementation from scratch, covering the model, loss function, minibatch SGD optimizer, and training loop, then applying it to synthetic data. Although deep learning frameworks can automate these steps, implementing them manually builds a deeper understanding that is essential for customizing models in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30dcf9",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9b380",
   "metadata": {},
   "source": [
    "Here, the model’s weights are initialized by sampling from a normal distribution with mean 0\n",
    "and standard deviation 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch(d2l.Module):  #@save\n",
    "    \"\"\"The linear regression model implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e796b7",
   "metadata": {},
   "source": [
    "Next, we define the model, specifying how the inputs and parameters are mathematically combined to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegressionScratch)  #@save\n",
    "def forward(self, X):\n",
    "    return torch.matmul(X, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84656f93",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2510b9",
   "metadata": {},
   "source": [
    "Here we use the squared loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegressionScratch)  #@save\n",
    "def loss(self, y_hat, y):\n",
    "    l = (y_hat - y) ** 2 / 2\n",
    "    return l.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ecc0c",
   "metadata": {},
   "source": [
    "### Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c6695",
   "metadata": {},
   "source": [
    "We define a SGD class and make an instance of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77567816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(d2l.HyperParameters):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegressionScratch)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return SGD([self.w, self.b], self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebbbdd6",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9612abb",
   "metadata": {},
   "source": [
    "With the parameters, loss function, model, and optimizer defined, we can now implement the main training loop to fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_batch(self, batch):\n",
    "    return batch\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def fit_epoch(self):\n",
    "    self.model.train()\n",
    "    for batch in self.train_dataloader:\n",
    "        loss = self.model.training_step(self.prepare_batch(batch))\n",
    "        self.optim.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "            if self.gradient_clip_val > 0:  # To be discussed later\n",
    "                self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "            self.optim.step()\n",
    "        self.train_batch_idx += 1\n",
    "    if self.val_dataloader is None:\n",
    "        return\n",
    "    self.model.eval()\n",
    "    for batch in self.val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            self.model.validation_step(self.prepare_batch(batch))\n",
    "        self.val_batch_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c16ad",
   "metadata": {},
   "source": [
    "Note that in general, both the number of epochs and the learning rate are `hyperparameters`. In general, setting hyperparameters is tricky and we will usually want to use a three-way split, one set for training, a second for hyperparameter selection, and the third reserved for the final evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionScratch(2, lr=0.03)\n",
    "data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(f'error in estimating w: {data.w - model.w.reshape(data.w.shape)}')\n",
    "    print(f'error in estimating b: {data.b - model.b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e905785",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1425f8",
   "metadata": {},
   "source": [
    "1. Experiment using different learning rates to find out how quickly the loss function value drops. Can you reduce the error by increasing the number of epochs of training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6529b4c",
   "metadata": {},
   "source": [
    "2. If the number of examples cannot be divided by the batch size, what happens to data_iter at the end of an epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e57e31",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605314e",
   "metadata": {},
   "source": [
    "In this section, we demonstrate a concise implementation of the linear regression model using high-level deep learning APIs. These abstractions streamline the code while preserving the same structure and logic as the from-scratch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205f7b1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93cdb31",
   "metadata": {},
   "source": [
    "Now we use a framework’s predefined layers, enabling us to focus on selecting and arranging the model’s layers without dealing with their low-level implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34396e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(d2l.Module):  #@save\n",
    "    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.LazyLinear(1)\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def forward(self, X):\n",
    "    return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5bcab",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dccbca",
   "metadata": {},
   "source": [
    "Again, we use pre-defined loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def loss(self, y_hat, y):\n",
    "    fn = nn.MSELoss()\n",
    "    return fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2ee48",
   "metadata": {},
   "source": [
    "### Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312137f6",
   "metadata": {},
   "source": [
    "Minibatch SGD is a common optimization method for training neural networks, and PyTorch’s optim module provides built-in support for it along with several variations of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b192e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de53d5b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8789f3c",
   "metadata": {},
   "source": [
    "Now that we have all the basic pieces in place, the training loop itself is the same as the one we implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(lr=0.03)\n",
    "data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a200b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def get_w_b(self):\n",
    "    return (self.net.weight.data, self.net.bias.data)\n",
    "w, b = model.get_w_b()\n",
    "\n",
    "print(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')\n",
    "print(f'error in estimating b: {data.b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ad1dc",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02ded4",
   "metadata": {},
   "source": [
    "Ex. 3:\n",
    "\n",
    "Consider the following definitions and then answer the question:\n",
    "- __Aggregate loss__ (sum): the minibatch loss is defined by the sum of the individual sample losses.\n",
    "- __Average loss__ (mean): the minibatch loss is defined as the mean of the sample losses (so the sum devided by the minibatch size).\n",
    "\n",
    "How would you need to change the learning rate if you replace the aggregate loss with an average loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e48141",
   "metadata": {},
   "source": [
    "Ex. 4:\n",
    "\n",
    "How do you access the gradient of the weights of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b35ecb",
   "metadata": {},
   "source": [
    "Ex. 5:\n",
    "\n",
    "Replace the squared loss with Huber’s robust loss function and run the training again. You can uncomment the line below to read more about the nn.HuberLoss available pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.HuberLoss?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
