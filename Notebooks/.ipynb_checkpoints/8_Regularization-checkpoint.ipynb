{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132cedb6",
   "metadata": {},
   "source": [
    "# Regularization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0961e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c998dd",
   "metadata": {},
   "source": [
    "## Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d76b0",
   "metadata": {},
   "source": [
    "We can illustrate the benefits of weight decay through a simple synthetic example.\n",
    "\n",
    "\n",
    "In this synthetic dataset, our label is given by an underlying linear function of our inputs, corrupted by Gaussian noise with zero mean and standard deviation 0.01. For illustrative purposes, we can make the effects of overfitting pronounced, by increasing the dimentionality to d=200, and working with a small training set with only 20 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e7115",
   "metadata": {},
   "source": [
    "### Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(d2l.DataModule):\n",
    "\n",
    "    def __init__(self, num_train, num_val, num_inputs, batch_size):\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, num_inputs)\n",
    "        noise = torch.randn(n, 1) * 0.01\n",
    "        w, b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "        self.y = torch.matmul(self.X, w) + b + noise\n",
    "        \n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader([self.X, self.y], train, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ec98d",
   "metadata": {},
   "source": [
    "Define the penalty term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecdf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w):\n",
    "    return (w ** 2).sum() / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a2afc",
   "metadata": {},
   "source": [
    "and a weight decay routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6118208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDecayScratch(d2l.LinearRegressionScratch):\n",
    "    \n",
    "    def __init__(self, num_inputs, lambd, lr, sigma=0.01):\n",
    "        super().__init__(num_inputs, lr, sigma)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        return (super().loss(y_hat, y) +\n",
    "                self.lambd * l2_penalty(self.w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc28670",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "\n",
    "def train_scratch(lambd):\n",
    "    model = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)\n",
    "    model.board.yscale='log' # Log scale for better visibility\n",
    "    trainer.fit(model, data)\n",
    "    print('L2 norm of w:', float(l2_penalty(model.w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ff0f4",
   "metadata": {},
   "source": [
    "### Training without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b907cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scratch(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a9ab5",
   "metadata": {},
   "source": [
    "### Train with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scratch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7c2bc",
   "metadata": {},
   "source": [
    "### Concise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDecay(d2l.LinearRegression):\n",
    "    \n",
    "    def __init__(self, wd, lr):\n",
    "        super().__init__(lr)\n",
    "        self.save_hyperparameters()\n",
    "        self.wd = wd\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD([\n",
    "            {'params': self.net.weight, 'weight_decay': self.wd},\n",
    "            {'params': self.net.bias}], lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WeightDecay(wd=3, lr=0.01)\n",
    "model.board.yscale='log'\n",
    "trainer.fit(model, data)\n",
    "print('L2 norm of w:', float(l2_penalty(model.get_w_b()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f872ab",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e618b8",
   "metadata": {},
   "source": [
    "### Ex.1 Try to improve this learning problem by:\n",
    "- Changing LR \n",
    "- Changing $\\lambda$\n",
    "- Increasing the number of epochs\n",
    "- Simulating more data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50c7dd",
   "metadata": {},
   "source": [
    "### Ex. 2 Try to use an L1 regularization\n",
    "\n",
    "What would the update equations look like if instead of $‚à•w‚à•^2$ we used $\\sum_ùëñ|ùë§_ùëñ|$ as our penalty of choice (l1 regularization)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbc22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
