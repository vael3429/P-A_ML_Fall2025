{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c691bed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import umap.umap_ as umap\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f1a3c",
   "metadata": {},
   "source": [
    "Import MNIST dataset and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0dc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: normalize and convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Download training and test data\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97f25e",
   "metadata": {},
   "source": [
    "Let's take a look at our training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043acfdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Training set size: {len(train_subset)} images\")\n",
    "print(f\"Validation set size: {len(val_subset)} images\")\n",
    "print(f\"Test set size:     {len(test_dataset)} images\")\n",
    "\n",
    "# Look at a single sample\n",
    "img, label = train_subset[0]\n",
    "print(f\"\\nSample image shape: {img.shape}\")\n",
    "print(f\"Sample label: {label}\")\n",
    "\n",
    "# Display a grid of samples\n",
    "def show_batch(dataloader, n=32):\n",
    "    imgs, labels = next(iter(dataloader))\n",
    "    grid = make_grid(imgs[:n], nrow=8, padding=2)\n",
    "    npimg = grid.numpy()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\"gray\")\n",
    "    plt.title(\"Batch of MNIST digits\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a3de6",
   "metadata": {},
   "source": [
    "Here we define our first architecture. It is a linear fully connected autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4f406-6a2e-4991-b867-8b5e701ed14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.BatchNorm1d(12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 10)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 12),\n",
    "            nn.BatchNorm1d(12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # Flatten the image\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 1, 28, 28) # Image is re-built\n",
    "        return x\n",
    "\n",
    "fc_autoencoder = FC_Autoencoder()\n",
    "\n",
    "print('Number of parameters:', sum(p.numel() for p in fc_autoencoder.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515484eb",
   "metadata": {},
   "source": [
    "Here we define a function which will tell us epoch by epoch the performance of the AE during training. We also define a function for training our autoencoder, which can be used also for other architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f1500-abdb-43b0-9b36-9a036c81bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_batch(x, y):\n",
    "    \"\"\"Compute mean cosine similarity between two image batches.\"\"\"\n",
    "    x = x.view(x.size(0), -1)\n",
    "    y = y.view(y.size(0), -1)\n",
    "    sim = F.cosine_similarity(x, y, dim=1)\n",
    "    return sim.mean().item()\n",
    "\n",
    "def train_autoencoder(\n",
    "    model, train_loader, val_loader, epochs=5, lr=1e-3, device=\"cpu\"\n",
    "):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_cos_sim_history = []\n",
    "    val_loss_history = []\n",
    "    val_cos_sim_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ===== TRAIN =====\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_cos_sim = 0.0\n",
    "\n",
    "        for imgs, _ in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = criterion(outputs, imgs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_cos_sim += cosine_similarity_batch(outputs.detach(), imgs)\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_cos = running_cos_sim / len(train_loader)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        train_cos_sim_history.append(epoch_train_cos)\n",
    "\n",
    "        # ===== VALIDATION =====\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_cos_sim = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                outputs = model(imgs)\n",
    "\n",
    "                loss = criterion(outputs, imgs)\n",
    "                val_loss += loss.item()\n",
    "                val_cos_sim += cosine_similarity_batch(outputs, imgs)\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_cos = val_cos_sim / len(val_loader)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        val_cos_sim_history.append(epoch_val_cos)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "            f\"- Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f} \"\n",
    "            f\"- Train CosSim: {epoch_train_cos:.4f}, Val CosSim: {epoch_val_cos:.4f}\"\n",
    "        )\n",
    "\n",
    "    # ===== PLOT =====\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\", color=\"tab:blue\")\n",
    "    ax1.plot(train_loss_history, label=\"Train Loss\", color=\"tab:blue\")\n",
    "    ax1.plot(val_loss_history,   label=\"Val Loss\",   color=\"tab:cyan\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"Cosine Similarity\", color=\"tab:red\")\n",
    "    ax2.plot(train_cos_sim_history, label=\"Train CosSim\", color=\"tab:red\", linestyle=\"--\")\n",
    "    ax2.plot(val_cos_sim_history,   label=\"Val CosSim\",   color=\"tab:pink\", linestyle=\"--\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title(\"Autoencoder Training & Validation\")\n",
    "    fig.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcb284",
   "metadata": {},
   "source": [
    "Let's train the autoencoder we previously built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30079207-ae99-4e15-af8b-2eb0017719ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_autoencoder(fc_autoencoder, train_loader, val_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9f8d3",
   "metadata": {},
   "source": [
    "A key step of evaluating the performance of an Autoencoder is to visually check how the reconstructed images look like wrt to the input images. It can help us understanding where the AE fails, or what to change in the architecture to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2eebf5-278b-45b7-ade8-4cff8ccb76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, dataloader, n=10):\n",
    "    model.eval()\n",
    "    imgs, _ = next(iter(dataloader))\n",
    "    imgs = imgs[:n]\n",
    "    with torch.no_grad():\n",
    "        recon = model(imgs)\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(imgs[i].squeeze(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        # Reconstructed\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(recon[i].squeeze(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstructions(fc_autoencoder, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3830a",
   "metadata": {},
   "source": [
    "Here we define a function to visualize the latent space distribution. If the dimensionality of the latent space is 2, it directly plots the latent space distribution, otherwise if the dimensionality is > 2, we use UMAP to reduce it and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc40f72-2023-4e3d-aad4-d40b53ba5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(model, dataloader, n_batches=50):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, lbls) in enumerate(dataloader):\n",
    "            \n",
    "            # Pass only through the encoder\n",
    "            z = model.encoder(imgs.view(imgs.size(0), -1)) if imgs.ndim == 4 and isinstance(model, FC_Autoencoder) \\\n",
    "                else model.encode(imgs)\n",
    "            latents.append(z.cpu())\n",
    "            labels.append(lbls)\n",
    "            \n",
    "            if i >= n_batches:  # limit batches for speed\n",
    "                break\n",
    "\n",
    "    latents = torch.cat(latents)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    if len(latents[0]) == 2:\n",
    "        additional = ''\n",
    "        latents_ = latents\n",
    "    else:\n",
    "        reducer = umap.UMAP(n_neighbors=20, min_dist=0.05, n_components=2,\n",
    "                    metric='euclidean', random_state=42)\n",
    "        print(latents.shape)\n",
    "        latents_ = reducer.fit_transform(latents)\n",
    "        additional = ', with UMAP reduction'\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(latents_[:, 0], latents_[:, 1], c=labels, cmap=\"tab10\", s=10, alpha=0.7)\n",
    "    plt.colorbar(scatter, ticks=range(10))\n",
    "    plt.title(f\"2D Latent Space (MNIST) {additional}\")\n",
    "    plt.xlabel(\"z₁\")\n",
    "    plt.ylabel(\"z₂\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_latent_space(fc_autoencoder, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e2d54",
   "metadata": {},
   "source": [
    "Now, as we are dealing with images, let's try a new AE architecture, based on Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485ee70-2484-49a0-803f-95a24ec72e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Autoencoder_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),   # -> (16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # -> (32, 7, 7)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Flatten -> 2D latent vector\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "        # Decoder: 2D -> (32, 7, 7)\n",
    "        self.fc3 = nn.Linear(10, 256)\n",
    "        self.fc4 = nn.Linear(256, 32 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # -> (16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),   # -> (1, 28, 28)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        z = self.fc2(x) # Final latent space\n",
    "        x = self.fc3(z)\n",
    "        x = self.fc4(x)\n",
    "        x = x.view(-1, 32, 7, 7)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    # helper for encoding only\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        z = self.fc2(x)\n",
    "        return z\n",
    "\n",
    "conv_autoencoder_2d = Conv_Autoencoder_2D()\n",
    "\n",
    "print('Number of parameters:', sum(p.numel() for p in conv_autoencoder_2d.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d0d88",
   "metadata": {},
   "source": [
    "Let's train the autoencoder with the function we defined before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521acd7b-d26d-48f9-b2e5-01c2db6250ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_autoencoder(conv_autoencoder_2d, train_loader, val_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7196a8f",
   "metadata": {},
   "source": [
    "...and visualize the reconstruction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979adb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_reconstructions(conv_autoencoder_2d, test_loader, n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aaa1db",
   "metadata": {},
   "source": [
    "Finally let's take a look at the latent space distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f94b8b-482c-40e2-aafc-a7e34a9ac476",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(conv_autoencoder_2d, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3463e0fd",
   "metadata": {},
   "source": [
    "Can we use the latent space to create new images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_random_latent_samples(model, n_samples=5, latent_dim=10):\n",
    "    model.eval()\n",
    "\n",
    "    # Sample from a normal distribution\n",
    "    z = torch.randn(n_samples, latent_dim)\n",
    "\n",
    "    # Decode to images\n",
    "    with torch.no_grad():\n",
    "        x_decoded = model.decoder(\n",
    "            model.fc4(\n",
    "                model.fc3(z)\n",
    "            ).view(-1, 32, 7, 7)\n",
    "        )\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(n_samples*2, 2))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(1, n_samples, i+1)\n",
    "        plt.imshow(x_decoded[i].squeeze().cpu(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(\"Random samples from latent space\")\n",
    "    plt.show()\n",
    "\n",
    "# use it\n",
    "show_random_latent_samples(conv_autoencoder_2d, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837b05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
