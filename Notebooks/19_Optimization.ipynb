{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8b886b",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from mpl_toolkits import mplot3d\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d8e13",
   "metadata": {},
   "source": [
    "## Introduction to concepts: Objective function and convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905261a",
   "metadata": {},
   "source": [
    "### Objective function and optimization Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351783f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # risk function\n",
    "    return x * torch.cos(np.pi * x)\n",
    "def g(x):\n",
    "    # empirical risk function\n",
    "    return f(x) + 0.2 * torch.cos(5 * np.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16229d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text, xy, xytext):  #@save\n",
    "    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,\n",
    "                           arrowprops=dict(arrowstyle='->'))\n",
    "x = torch.arange(0.5, 1.5, 0.01)\n",
    "d2l.set_figsize((4.5, 2.5))\n",
    "d2l.plot(x, [f(x), g(x)], 'x', 'risk')\n",
    "annotate('min of\\nempirical risk', (1.0, -1.2), (0.5, -1.1))\n",
    "annotate('min of risk', (1.1, -1.05), (0.95, -0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844afa80",
   "metadata": {},
   "source": [
    "#### Local minima are problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f394414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.arange(-1.0, 2.0, 0.01)\n",
    "d2l.plot(x, [f(x), ], 'x', 'f(x)')\n",
    "annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))\n",
    "annotate('global minimum', (1.1, -0.95), (0.6, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60d1ef",
   "metadata": {},
   "source": [
    "#### Saddle points are problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-2.0, 2.0, 0.01)\n",
    "d2l.plot(x, [x**3], 'x', 'f(x)')\n",
    "annotate('saddle point', (0, -0.2), (-0.52, -5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdac26",
   "metadata": {},
   "source": [
    "Saddle points in higher dimensions are even more insidious, as the example below shows.\n",
    "\n",
    "(0,0) This is a maximum with respect to ð‘¦ and a minimum with respect to ð‘¥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d984b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = torch.meshgrid(\n",
    "    torch.linspace(-1.0, 1.0, 101), torch.linspace(-1.0, 1.0, 101))\n",
    "z = x**2 - y**2\n",
    "ax = d2l.plt.figure().add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})\n",
    "ax.plot([0], [0], [0], 'rx', zorder=10)  # saddle point\n",
    "ticks = [-1, 0, 1]\n",
    "d2l.plt.xticks(ticks)\n",
    "d2l.plt.yticks(ticks)\n",
    "ax.set_zticks(ticks)\n",
    "d2l.plt.xlabel('x')\n",
    "d2l.plt.ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1bbc34",
   "metadata": {},
   "source": [
    "#### In general:\n",
    "\n",
    "Assume that the input of a function is a ð‘˜-dimensional vector and its output is a scalar, its *Hessian matrix* will have ð‘˜ eigenvalues. \n",
    "The solution of the function could be a local minimum, a local maximum, or a saddle point at a position where the function gradient is\n",
    "zero:\n",
    "\n",
    "- When the eigenvalues of the functionâ€™s Hessian matrix at the zero-gradient position are\n",
    "all positive, we have a local minimum for the function.\n",
    "- When the eigenvalues of the functionâ€™s Hessian matrix at the zero-gradient position are\n",
    "all negative, we have a local maximum for the function.\n",
    "- When the eigenvalues of the functionâ€™s Hessian matrix at the zero-gradient position are\n",
    "negative and positive, we have a saddle point for the function.\n",
    "\n",
    "For high-dimensional problems the likelihood that at least some of the eigenvalues are neg-\n",
    "ative is quite high. This makes saddle points more likely than local minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45d09c",
   "metadata": {},
   "source": [
    "#### vanishing gradients are problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-2.0, 5.0, 0.01)\n",
    "d2l.plot(x, [torch.tanh(x)], 'x', 'f(x)')\n",
    "annotate('vanishing gradient', (4, 1), (2, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7cc363",
   "metadata": {},
   "source": [
    "### Convexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a45075",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from mpl_toolkits import mplot3d\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 0.5 * x**2  # Convex\n",
    "g = lambda x: torch.cos(np.pi * x)  # Nonconvex\n",
    "h = lambda x: torch.exp(0.5 * x)  # Convex\n",
    "\n",
    "\n",
    "x, segment = torch.arange(-2, 2, 0.01), torch.tensor([-1.5, 1])\n",
    "\n",
    "# i want to illustrate that the functions are convex or not convex by plotting the line segment between two points \n",
    "# and then comparing lambda*f(x1) + (1 - lambda)*f(x2) to the function value at the midpoint (Lambda = 0.5: (x1 + x2) / 2)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for ax, func in zip(axes, [f, g, h]):\n",
    "    ax.plot(x.numpy(), func(x).numpy())\n",
    "    ax.plot(segment.numpy(), func(segment).numpy(), 'r-')\n",
    "    midpoint = (segment[0] + segment[1]) / 2\n",
    "    ax.plot(midpoint.numpy(), func(midpoint).numpy(), 'bo', label=r'$f(\\lambda x_1 + (1 - \\lambda) x_2)$')\n",
    "    ax.hlines(0.5 * (func(segment[0]) + func(segment[1])), xmin=segment[0].item(), xmax=segment[1].item(), \n",
    "              color='orange', linestyle='--', label=r'$\\lambda f(x_1) + (1 - \\lambda) f(x_2)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbe622",
   "metadata": {},
   "source": [
    "As expected, the cosine function is nonconvex, whereas the parabola and the exponential function are. Note that the exponential doesn't have a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8fdbe",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):  # Objective function\n",
    "    return x ** 2\n",
    "def f_grad(x):  # Gradient (derivative) of the objective function\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(eta, f_grad):\n",
    "    x = 10\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x = x - eta * f_grad(x)\n",
    "        results.append(float(x))\n",
    "    print(f'epoch 10, x: {x:f}')\n",
    "    return results\n",
    "\n",
    "results = gd(0.2, f_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = torch.arange(-n, n, 0.01)\n",
    "    d2l.set_figsize()\n",
    "    d2l.plot([f_line, results], [[f(x) for x in f_line], [\n",
    "        f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
    "    \n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c453abb",
   "metadata": {},
   "source": [
    "----------------- YOUR TURN! -----------------\n",
    "\n",
    "Now try to run again the gd() function using first a smaller eta (e.g. eta = 0.05) and then again with a larger eta (e.g. eta = 1.1).\n",
    "\n",
    "Plot and consider what you see and discuss what that means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d558816",
   "metadata": {},
   "source": [
    "### Local Minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "def f(x):  # Objective function\n",
    "    return x * torch.cos(c * x)\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return torch.cos(c * x) - c * x * torch.sin(c * x)\n",
    "\n",
    "show_trace(gd(2, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529960c",
   "metadata": {},
   "source": [
    "### Multivariate Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need two more helper functions. The first uses an update function and applies it \n",
    "# 20 times to the initial value. \n",
    "# The second helper visualizes the trajectory of x.\n",
    "\n",
    "def train_2d(trainer, steps=20, f_grad=None):  #@save\n",
    "    \"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n",
    "    # `s1` and `s2` are internal state variables that will be used in Momentum, adagrad, RMSProp\n",
    "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
    "    results = [(x1, x2)]\n",
    "    for i in range(steps):\n",
    "        if f_grad:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n",
    "        else:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n",
    "        results.append((x1, x2))\n",
    "    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n",
    "    return results\n",
    "\n",
    "def show_trace_2d(f, results):  #@save\n",
    "    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e', markersize=3, linewidth=1)\n",
    "    x1, x2 = torch.meshgrid(torch.arange(-5.5, 2.0, 0.1),\n",
    "                          torch.arange(-3.0, 2.0, 0.1), indexing='ij')\n",
    "    d2l.plt.contourf(x1, x2, f(x1, x2), levels=20, cmap='Blues_r')\n",
    "    d2l.plt.xlabel('x1')\n",
    "    d2l.plt.ylabel('x2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 2D objective function and its gradient:\n",
    "# x = (x1, x2)\n",
    "# f(x) = x1^2 + 2 * x2^2\n",
    "# f'(x) = (2 * x1, 4 * x2)\n",
    "\n",
    "def f_2d(x1, x2):  # Objective function\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_2d_grad(x1, x2):  # Gradient of the objective function\n",
    "    return (2 * x1, 4 * x2)\n",
    "\n",
    "def gd_2d(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_2d_grad(x1, x2)\n",
    "    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n",
    "\n",
    "\n",
    "eta = 0.1\n",
    "show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d43dfd",
   "metadata": {},
   "source": [
    "### Adaptive Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4267425f",
   "metadata": {},
   "source": [
    "What if we could determine ðœ‚ automatically or get rid of having to select a learning rate at all? A possibility is to look at second-order methods:\n",
    "- Newton's method\n",
    "- Preconditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.5)\n",
    "def f(x):  # Objective function\n",
    "    return torch.cosh(c * x)\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return c * torch.sinh(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return c**2 * torch.cosh(c * x)\n",
    "\n",
    "def newton(eta=1):\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * f_grad(x) / f_hess(x)\n",
    "        results.append(float(x))\n",
    "    print('epoch 10, x:', x)\n",
    "    return results\n",
    "\n",
    "show_trace(newton(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ddc85",
   "metadata": {},
   "source": [
    "Let's try with a non-convex function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85baf1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return x * torch.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return torch.cos(c * x) - c * x * torch.sin(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return - 2 * c * torch.sin(c * x) - x * c**2 * torch.cos(c * x)\n",
    "\n",
    "show_trace(newton(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(newton(0.5), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cd953",
   "metadata": {},
   "source": [
    "## Stocastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2caae",
   "metadata": {},
   "source": [
    "We will compare SGD with gradient descent by adding random noise with a mean of 0\n",
    "and a variance of 1 to the gradient to simulate a stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):  # Objective function\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_grad(x1, x2):  # Gradient of the objective function\n",
    "    return 2 * x1, 4 * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aaa0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    # Simulate noisy gradient\n",
    "    g1 += torch.normal(0.0, 1, (1,)).item()\n",
    "    g2 += torch.normal(0.0, 1, (1,)).item()\n",
    "    eta_t = eta * lr() # lr is defined outside this function\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d10bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_lr():\n",
    "    return 1    \n",
    "\n",
    "eta = 0.1\n",
    "lr = constant_lr\n",
    "show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a4650",
   "metadata": {},
   "source": [
    "Even after 50 steps the quality is still not so good. Even worse, it will not improve after additional steps (you are encouraged to experiment with a larger number of steps to confirm this). \n",
    "\n",
    "This leaves us with the only alternative: change the learning rate ðœ‚.\n",
    "However, if we pick this too small, we will not make any meaningful progress initially. On the other hand, if we pick it too large, we will not get a good solution, as seen above. \n",
    "\n",
    "The only way to resolve these conflicting goals is to reduce the learning rate dynamically as optimization progresses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ffb54",
   "metadata": {},
   "source": [
    "### Dynamic Learning Rate (Scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98731580",
   "metadata": {},
   "source": [
    "We aleready defined the constant lr function. Now we define an exponential lr and a polinomial lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_lr():\n",
    "    # Global variable that is defined outside this function and updated inside\n",
    "    global t\n",
    "    t += 1\n",
    "    return math.exp(-0.1 * t)\n",
    "\n",
    "t=1\n",
    "lr = exponential_lr\n",
    "show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))\n",
    "\n",
    "#Note: this can lead to premature stopping, failing to converge to the optimal solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e350d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_lr():\n",
    "    # Global variable that is defined outside this function and updated inside\n",
    "    global t\n",
    "    t += 1\n",
    "    return (1 + 0.1 * t) ** (-0.5)\n",
    "t=1\n",
    "lr = polynomial_lr\n",
    "show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66250974",
   "metadata": {},
   "source": [
    "## Minibatch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0a22e",
   "metadata": {},
   "source": [
    "### Again: computational time matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf8fb17",
   "metadata": {},
   "source": [
    "To keep matters simple, consider matrix-matrix multiplication, say A = BC. We have a number of options for calculating A. For instance, we could try the following:\n",
    "\n",
    "- We could compute Að‘– ð‘— = Bð‘–,:C:, ð‘— , i.e., we could compute it elementwise by means of dot products.\n",
    "- We could compute A:, ð‘— = BC:, ð‘— , i.e., we could compute it one column at a time. Likewise we could compute A one row Að‘–,: at a time.\n",
    "- We could simply compute A = BC.\n",
    "- We could break B and C into smaller block matrices and compute A one block at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros(256, 256)\n",
    "B = torch.randn(256, 256)\n",
    "C = torch.randn(256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:  #@save\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "    \n",
    "timer = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab14ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute A = BC one element at a time\n",
    "timer.start()\n",
    "for i in range(256):\n",
    "    for j in range(256):\n",
    "        A[i, j] = torch.dot(B[i, :], C[:, j])\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b119514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute A = BC one column at a time\n",
    "timer.start()\n",
    "for j in range(256):\n",
    "    A[:, j] = torch.mv(B, C[:, j])\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute A = BC in one go\n",
    "timer.start()\n",
    "A = torch.mm(B, C)\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319ddc2",
   "metadata": {},
   "source": [
    " In short, it is highly advisable to use vectorization (and matrices) whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122fe90",
   "metadata": {},
   "source": [
    "### Implementation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f481ef4",
   "metadata": {},
   "source": [
    "Letâ€™s have a look at how minibatches are efficiently generated from data. In the following we use a dataset developed by NASA to test the wing noise from different aircraft to compare these optimization algorithms. For convenience we only use the first 1500 examples. The data is whitened for preprocessing, i.e., we remove the mean and rescale the variance to 1 per coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed472997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n",
    "                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n",
    "#@save\n",
    "def get_data_ch11(batch_size=10, n=1500):\n",
    "    data = np.genfromtxt(d2l.download('airfoil'),\n",
    "                         dtype=np.float32, delimiter='\\t')\n",
    "    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))\n",
    "    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\n",
    "                               batch_size, is_train=True)\n",
    "    return data_iter, data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ebc4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, states, hyperparams):\n",
    "    for p in params:\n",
    "        p.data.sub_(hyperparams['lr'] * p.grad) # this is the same as p.data = p.data - hyperparams['lr'] *  p.grad\n",
    "        p.grad.data.zero_() # reset gradient to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_ch11(trainer_fn, states, hyperparams, data_iter,\n",
    "               feature_dim, num_epochs=2):\n",
    "    # Initialization\n",
    "    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\n",
    "                     requires_grad=True)\n",
    "    b = torch.zeros((1), requires_grad=True)\n",
    "    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
    "    # Train\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs] ) #\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            l = loss(net(X), y).mean()\n",
    "            l.backward()\n",
    "            trainer_fn([w, b], states, hyperparams)\n",
    "            n += X.shape[0] # number of examples processed\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (d2l.evaluate_loss(net, data_iter, loss),))\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n",
    "    return timer.cumsum(), animator.Y[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82013a6",
   "metadata": {},
   "source": [
    "Letâ€™s see how optimization proceeds for batch gradient descent. This can be achieved by setting the minibatch size to 1500 (i.e., to the total number of examples).\n",
    "\n",
    "As a result the model parameters are updated only once per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a01a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(lr, batch_size, num_epochs=2):\n",
    "    data_iter, feature_dim = get_data_ch11(batch_size)\n",
    "    return train_ch11(\n",
    "        sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)\n",
    "\n",
    "# GRADIENT DESCENT example (passing the full dataset each iteration)\n",
    "gd_res = train_sgd(0.05, 1500, 50)\n",
    "\n",
    "# the loss function stops decreasing very soon, and eventually starts increasing again. This is because the learning rate is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c75843",
   "metadata": {},
   "source": [
    "How does the progress look like after 10 epochs?\n",
    "\n",
    "------------------------------------------------\n",
    "When the batch size equals 1, we use stochastic gradient descent for optimization. For simplicity of implementation we picked a constant (albeit small) learning rate. \n",
    "\n",
    "In stochastic gradient descent, the model parameters are updated whenever an example is processed. In our case this amounts to 1500 updates per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCHASTIC GRADIENT DESCENT example (passing one example each iteration)\n",
    "sgd_res = train_sgd(0.005, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf8a26",
   "metadata": {},
   "source": [
    "How's the optimization going now? How does the time to run an epoch compare to the gradient descent?\n",
    "\n",
    "-------------------------------------------------\n",
    "Finally, when the batch size equals 100, we use minibatch stochastic gradient descent for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ac6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-BATCH GRADIENT DESCENT example (passing 40 examples each iteration)\n",
    "mini1_res = train_sgd(0.05, 100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0723570",
   "metadata": {},
   "source": [
    "minibatch SGD is the most efficient way to optimize the model parameters: better performances and more efficient computationally.\n",
    "\n",
    "Try to change the batch size and see how the time to compute an epoch changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini2_res = train_sgd(.05, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize([6, 3])\n",
    "d2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),\n",
    "         'time (sec)', 'loss', xlim=[1e-3, 10],\n",
    "         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\n",
    "d2l.plt.gca().set_xscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22dd5a",
   "metadata": {},
   "source": [
    "### Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced4393",
   "metadata": {},
   "source": [
    "In Gluon, we can use the Trainer class to call optimization algorithms. This is used to im- plement a generic training function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c209d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):\n",
    "    # Initialization\n",
    "    net = nn.Sequential(nn.Linear(5, 1))\n",
    "    def init_weights(module):\n",
    "        if type(module) == nn.Linear:\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "    optimizer = trainer_fn(net.parameters(), **hyperparams) # Can be whatever optimizer you want\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            out = net(X)\n",
    "            y = y.reshape(out.shape)\n",
    "            l = loss(out, y)\n",
    "            l.mean().backward()\n",
    "            optimizer.step()\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                # `MSELoss` computes squared error without the 1/2 factor\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (d2l.evaluate_loss(net, data_iter, loss) / 2,))\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, _ = get_data_ch11(10)\n",
    "trainer = torch.optim.SGD\n",
    "train_concise_ch11(trainer, {'lr': 0.01}, data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e4a3e",
   "metadata": {},
   "source": [
    "## Other famous Optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068717fb",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ac00b",
   "metadata": {},
   "source": [
    "Accelerated gradient methods, such as gradients with momentum, replace the gradient computation by a â€œleaky averageâ€, averaged over multiple past gradients. The new gradient replacement no longer points into the direction of steepest descent on a particular instance any longer but rather in the direction of a weighted average of past gradients.\n",
    "\n",
    "This is particularly useful when we have ill-conditioned Problems like the one illlustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4\n",
    "\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def gd_2d(x1, x2, s1, s2):\n",
    "    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n",
    "\n",
    "show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444644dd",
   "metadata": {},
   "source": [
    "By construction, the gradient in the $ð‘¥_2$ direction is much higher and changes much more rapidly than in the horizontal $ð‘¥_1$ direction. Thus we are stuck between two undesirable choices: if we pick a small learning rate we ensure that the solution does not diverge in the ð‘¥2 direction but we are saddled with slow convergence in the ð‘¥1 direction. Conversely, with a large learning rate we progress rapidly in the ð‘¥1 direction but diverge in ð‘¥2.\n",
    "\n",
    "See what happen with a slightly higher LR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.6\n",
    "\n",
    "show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f57296",
   "metadata": {},
   "source": [
    "Using \\( $v_t$ \\) instead of the gradient \\( $g_t$ \\) yields the following update equations:  \n",
    "\n",
    "\n",
    "$v_t \\leftarrow \\beta v_{t-1} + g_{t, t-1}, \\quad$\n",
    "$x_t \\leftarrow x_{t-1} - \\eta_t v_t.$\n",
    "\n",
    "Note that for \\( $\\beta = 0$ \\) we recover regular gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_2d(x1, x2, v1, v2):\n",
    "    v1 = beta * v1 + 0.2 * x1\n",
    "    v2 = beta * v2 + 4 * x2\n",
    "    return x1 - eta * v1, x2 - eta * v2, v1, v2\n",
    "\n",
    "eta, beta = 0.6, 0.5\n",
    "show_trace_2d(f_2d, d2l.train_2d(momentum_2d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa20dc61",
   "metadata": {},
   "source": [
    "#### Concise impelmentation of Momentum in SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "# torch.optim.SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False, fused=None)\n",
    "\n",
    "d2l.train_concise_ch11(trainer, {'lr': 0.005, 'momentum': 0.9}, data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdaadfb",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3dc6bc",
   "metadata": {},
   "source": [
    "Letâ€™s begin by considering learning problems with features that occur infrequently (aka training on sparse features).\n",
    "\n",
    "Parameters associated with infrequent features only receive meaningful updates whenever these features occur. Given a decreasing learning rate we might end up in a situation where the parameters for common features converge rather quickly to their optimal values, whereas for infrequent features we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate either decreases too slowly for frequent features or too quickly for infrequent ones.\n",
    "\n",
    "A possible hack to redress this issue would be to count the number of times we see a particular feature and to use this as a clock for adjusting learning rates.\n",
    "\n",
    "Adagrad by Duchi et al. (2011) addresses this by replacing the rather crude counter ð‘ (ð‘–, ð‘¡) by an aggregate of the squares of previously observed gradients. In particular, it uses $ð‘ (ð‘–, ð‘¡ ) = ð‘ (ð‘–, ð‘¡-1) + (ðœ•_ð‘– ð‘“ (x))^2$ as a means to adjust the learning rate. \n",
    "\n",
    "The algorithm looks like:\n",
    "\n",
    "$g_t = \\frac{\\partial \\ell(y_t, f(x_t, w))}{\\partial w}$\n",
    "\n",
    "$s_t = s_{t-1} + g_t^2$\n",
    "\n",
    "$w_t = w_{t-1} - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} \\cdot g_t$\n",
    "\n",
    " ðœ‚ is the learning rate and ðœ– is an additive constant that ensures that we do not divide by 0.\n",
    "\n",
    "\n",
    "We use the same problem as before (the ill-conditioned one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad_2d(x1, x2, s1, s2):\n",
    "    eps = 1e-6\n",
    "    g1, g2 = 0.2 * x1, 4 * x2\n",
    "    s1 += g1 ** 2\n",
    "    s2 += g2 ** 2\n",
    "    x1 -= eta / math.sqrt(s1 + eps) * g1\n",
    "    x2 -= eta / math.sqrt(s2 + eps) * g2\n",
    "    return x1, x2, s1, s2\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "\n",
    "eta = 0.4\n",
    "show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef718a6c",
   "metadata": {},
   "source": [
    "As we increase the learning rate to 2 we see much better behavior. This already indicates that the decrease in learning rate might be rather aggressive, even in the noise-free case and we need to ensure that parameters converge appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f95968",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1\n",
    "show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321fe155",
   "metadata": {},
   "source": [
    "#### Concise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.Adagrad\n",
    "d2l.train_concise_ch11(trainer, {'lr': 0.1}, data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cce2a9",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b03213",
   "metadata": {},
   "source": [
    "One of the key issues of Adagrad is that the learning rate decreases at a predefined schedule of effectively O($1/\\sqrt{ð‘¡}$). While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Adagrad accumulates the squares of the gradient $g_ð‘¡$ into a state vector $s_ð‘¡ = s_{ð‘¡âˆ’1} + g_ð‘¡^2$. As a result $s_ð‘¡$ keeps on growing without bound due to the lack of normalization, essentially linearly as the algorithm converges.\n",
    "\n",
    "One way of fixing this problem would be to use $s_ð‘¡/ð‘¡$. Unfortunately it might take a very long time until the limit behavior starts to matter since the procedure remembers the full trajectory of values. An alternative is to use a leaky average in the same way we used in the momentum method.\n",
    "\n",
    "Tieleman and Hinton (2012) proposed the RMSProp algorithm as a simple fix to decouple rate scheduling from coordinate-adaptive learning rates.\n",
    "\n",
    "$s_t \\leftarrow \\gamma s_{t-1} + (1 - \\gamma) g_t^2$\n",
    "\n",
    "$x_t \\leftarrow x_{t-1} - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} \\,\\odot\\, g_t$\n",
    "\n",
    "constant ðœ– > 0 is typically set to $10^{âˆ’6}$ to ensure that we do not suffer from division by zero or overly large step sizes. \n",
    "\n",
    "As per the $\\beta$ hyperparameter of momentum, we use 1+ ð›¾ + ð›¾^2+ ... = 1/(1 - ð›¾). Hence the sum of weights is normalized to 1 with a half-life time of an observation of 1/ð›¾.\n",
    "\n",
    "\n",
    "# -------------------\n",
    "\n",
    "#### AdaGrad: â€œRemember all the gradients ever seen.â€\n",
    "- great for convex or sparse problems.\n",
    "- bad for long training â€” too cautious.\n",
    "#### RMSProp: â€œForget gradually with an exponential decay.â€\n",
    "- keeps adapting indefinitely.\n",
    "- designed for non-stationary deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280aae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_2d(x1, x2, s1, s2):\n",
    "    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6\n",
    "    s1 = gamma * s1 + (1 - gamma) * g1 ** 2\n",
    "    s2 = gamma * s2 + (1 - gamma) * g2 ** 2\n",
    "    x1 -= eta / math.sqrt(s1 + eps) * g1\n",
    "    x2 -= eta / math.sqrt(s2 + eps) * g2\n",
    "    return x1, x2, s1, s2\n",
    "\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "eta, gamma = 0.4, 0.9\n",
    "show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f7473",
   "metadata": {},
   "source": [
    "#### Concise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1321235",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.RMSprop\n",
    "d2l.train_concise_ch11(trainer, {'lr': 0.01, 'alpha': 0.9},\n",
    "data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22fa37",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb7f97",
   "metadata": {},
   "source": [
    "Adam (Kingma and Ba, 2014) combines all these techniques into one efficient learning algorithm. As expected, this is an algorithm that has become rather popular as one of the more robust and effective optimization algorithms to use in deep learning.\n",
    "\n",
    "One of the key components of Adam is that it uses exponential weighted moving averages (also known as leaky averaging) to obtain an estimate of both the momentum and also the\n",
    "second moment of the gradient. \n",
    "\n",
    "It uses two state variables:\n",
    "\n",
    "$v_t \\leftarrow \\beta_1 v_{t-1} + (1 - \\beta_1) g_t$\n",
    "\n",
    "$s_t \\leftarrow \\beta_2 s_{t-1} + (1 - \\beta_2) g_t^2$\n",
    "\n",
    " ð›½1 and ð›½2 are nonnegative weighting parameters. Common choices for them are ð›½1 = 0.9 and ð›½2 = 0.999. That is, the variance estimate moves much more slowly than the\n",
    "momentum term.\n",
    "\n",
    "For reasons related to initial biases induced by v0=s0=0, it is better to use normalized state variables:\n",
    "\n",
    "$\\hat{v}_t = v_t / (1 -  \\beta_1^t), \\quad$\n",
    "$\\hat{s}_t = s_t / (1 -  \\beta_2^t)$\n",
    "\n",
    "then we rescale the gradient\n",
    "\n",
    "\n",
    "$g'_t = (\\eta \\hat{v}_t) / (\\sqrt{\\hat{s}_t} +\\epsilon)$\n",
    "\n",
    "finally the update is the following\n",
    "\n",
    "$x_t \\leftarrow x_{t-1} - g'_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5486b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_adam_states(feature_dim):\n",
    "    v_w, v_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\n",
    "    s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\n",
    "    return ((v_w, s_w), (v_b, s_b))\n",
    "\n",
    "def adam(params, states, hyperparams):\n",
    "    beta1, beta2, eps = 0.9, 0.999, 1e-6\n",
    "    for p, (v, s) in zip(params, states):\n",
    "        with torch.no_grad():\n",
    "            v[:] = beta1 * v + (1 - beta1) * p.grad\n",
    "            s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad)\n",
    "            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n",
    "            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n",
    "            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)\n",
    "        p.grad.data.zero_()\n",
    "    hyperparams['t'] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066a64d",
   "metadata": {},
   "source": [
    "#### Concise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.Adam\n",
    "d2l.train_concise_ch11(trainer, {'lr': 0.01}, data_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f46be4",
   "metadata": {},
   "source": [
    "#### Yogi (study on your own)\n",
    "\n",
    "One of the problems of Adam is that it can fail to converge even in convex settings when the second moment estimate in sð‘¡ blows up. Infact, in \n",
    "\n",
    "$s_t \\leftarrow \\beta_2 s_{t-1} + (1 - \\beta_2) g_t^2$\n",
    "\n",
    "if the gradient magnitude changes drastically, Adamâ€™s $s_t$ can get too large, causing an overly small effective learning rate and slow convergence.\n",
    "\n",
    "As a fix Zaheer et al. (2018) proposed a refined update (and initialization) for sð‘¡.\n",
    "\n",
    "$s_t = s_{t-1} + (1 - \\beta_2) \\; \\text{sign}(s_{t-1} - g_t^2) \\cdot g_t^2$\n",
    "\n",
    "where the **sign function** sign(x) is defined as:\n",
    "\n",
    "- $+1$ if $x > 0$  \n",
    "- $0$ if $x = 0$  \n",
    "- $-1$ if $x < 0$\n",
    "\n",
    "**Key differences:**\n",
    "\n",
    "- Instead of always increasing $s_t$, the update moves $s_t$ toward $g_t^2$ in a controlled way.  \n",
    "- If $g_t^2$ is smaller than $s_{t-1}$, the `sign` term ensures $s_t$ decreases.  \n",
    "- If $g_t^2$ is larger, $s_t$ increases â€” but gradually.  \n",
    "- This prevents $s_t$ from growing too fast and stabilizes the learning rate over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yogi(params, states, hyperparams):\n",
    "    beta1, beta2, eps = 0.9, 0.999, 1e-3\n",
    "    for p, (v, s) in zip(params, states):\n",
    "        with torch.no_grad():\n",
    "            v[:] = beta1 * v + (1 - beta1) * p.grad\n",
    "            s[:] = s + (1 - beta2) * torch.sign(\n",
    "                torch.square(p.grad) - s) * torch.square(p.grad)\n",
    "            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n",
    "            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n",
    "            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)\n",
    "        p.grad.data.zero_()\n",
    "    hyperparams['t'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90298bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n",
    "d2l.train_ch11(yogi, init_adam_states(feature_dim),\n",
    "               {'lr': 0.01, 't': 1}, data_iter, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17dc94",
   "metadata": {},
   "source": [
    "## Simple LR Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195833e9",
   "metadata": {},
   "source": [
    "### Learning Rate Schedulers in PyTorch\n",
    "\n",
    "In PyTorch, **learning rate schedulers** adjust the optimizerâ€™s learning rate during training to improve convergence and avoid getting stuck in poor local minima.\n",
    "\n",
    "Schedulers are available in [`torch.optim.lr_scheduler`](https://docs.pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) and can be used with any optimizer.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **Common Schedulers**\n",
    "\n",
    "##### 1. StepLR\n",
    "- Reduces the learning rate by a factor every fixed number of epochs.\n",
    "- Use case: Simple decay strategy.\n",
    "- Formula: `lr = lr * gamma` every `step_size` epochs. \n",
    "\n",
    "##### 2. MultiStepLR\n",
    "- Like StepLR, but with multiple specific decay points.\n",
    "- Use case: Predefined decay epochs.\n",
    "- Example code belo (Note: the network is not defined here, it could be any network. This is just to illustrate how the scheduler works)\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "net = net_fn()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "scheduler = lr_scheduler.MultiStepLR(trainer, milestones=[15, 30], gamma=0.5)\n",
    "def get_lr(trainer, scheduler):\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    trainer.step()\n",
    "    scheduler.step()\n",
    "    return lr\n",
    "```\n",
    "\n",
    "##### 3. ExponentialLR\n",
    "- Multiplies the learning rate by gamma every epoch.\n",
    "- Use case: Smooth exponential decay.\n",
    "\n",
    "##### 4. CosineAnnealingLR\n",
    "- Learning rate follows a cosine curve from `max_lr` to `min_lr`.\n",
    "- Use case: Gradual restarts or cyclic schedules.\n",
    "\n",
    "##### 5. ReduceLROnPlateau\n",
    "- Reduces the learning rate when a monitored metric stops improving.\n",
    "- Use case: Validation-based adjustments.\n",
    "\n",
    "##### 6. OneCycleLR\n",
    "- Increases then decreases the learning rate in one cycle.\n",
    "- Use case: Fast convergence (Smith, 2018).\n",
    "\n",
    "##### 7. LambdaLR\n",
    "- Custom learning rate schedule via a lambda function.\n",
    "- Use case: Fully customized decay logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385e425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
