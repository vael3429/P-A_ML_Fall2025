{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f081c6",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9ffe7",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25879ea",
   "metadata": {},
   "source": [
    "Activation functions determine if a neuron should fire by applying a bias after computing the weighted sum of its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12614cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6a6ce",
   "metadata": {},
   "source": [
    "### ReLU Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ef856",
   "metadata": {},
   "source": [
    "The ReLU function keeps positive values as they are, while replacing all negative values with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "y = torch.relu(x)\n",
    "\n",
    "d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8512ad0",
   "metadata": {},
   "source": [
    "We plot the derivative of the ReLU function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "\n",
    "d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff87e6f",
   "metadata": {},
   "source": [
    " ------------------------ YOUR TURN -----------------------\n",
    " \n",
    "Implement a pReLU activation function and plot varying the alpha parameter.\n",
    "\n",
    "PReLU = Parametric ReLU\n",
    "\n",
    "PReLU is a variant of ReLU that allows the slope of the negative part to be learned during training.\n",
    "It is defined as \n",
    "* `f(x) = max(0, x)` for `x >= 0` and \n",
    "* `f(x) = weight * x` for `x < 0` \n",
    "\n",
    "where weight is a learnable parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36f562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7b9a2c5",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bb963",
   "metadata": {},
   "source": [
    "The sigmoid function transforms those inputs whose values lie in the domain $\\mathbb{R}$, to outputs that lie on the interval (0, 1). The form of the function is:\n",
    "$$sigmoid(x) = \\frac{1}{1+ e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc457d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.sigmoid(x)\n",
    "\n",
    "d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81dba2",
   "metadata": {},
   "source": [
    "The derivative of sigmoid function could be calculated as below:\n",
    "$$\\frac{d}{dx}sigmoid(x) =  sigmoid(x)(1-sigmoid(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ca35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear out previous gradients\n",
    "x.grad.data.zero_()\n",
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "\n",
    "d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5), ylim=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef5df4",
   "metadata": {},
   "source": [
    "### Tanh Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df42bac",
   "metadata": {},
   "source": [
    "Similar to the sigmoid, the tanh (hyperbolic tangent) function compresses its inputs, mapping them to outputs within the interval \n",
    "$(âˆ’1,1)$:\n",
    "$$tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tanh(x)\n",
    "d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c6fa4",
   "metadata": {},
   "source": [
    "The derivative of the tanh function is:\n",
    "$$\\frac{d}{dx}tanh(x) = 1 - tanh^{2}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear out previous gradients\n",
    "x.grad.data.zero_()\n",
    "y.backward(torch.ones_like(x),retain_graph=True)\n",
    "d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1546c",
   "metadata": {},
   "source": [
    "## Implementation of Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4472af",
   "metadata": {},
   "source": [
    "### From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be8970",
   "metadata": {},
   "source": [
    "#### Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18e71a",
   "metadata": {},
   "source": [
    "Fashion-MNIST is treated as a classification task with 784 input features and 10 classes. We implement an MLP with one hidden layer of 256 units, noting that layer size and depth are tunable hyperparameters. Each layer requires a weight matrix and bias vector, along with memory for their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16764a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPScratch(d2l.Classifier):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_hiddens))\n",
    "        self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\n",
    "        self.b2 = nn.Parameter(torch.zeros(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051a107",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f67a3",
   "metadata": {},
   "source": [
    "To fully understand its mechanics, we will code the ReLU activation manually instead of relying on the built-in relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e912a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c5e1c",
   "metadata": {},
   "source": [
    "Because we ignore spatial structure, each 2D image is flattened into a vector of length num_inputs. With autograd handling differentiation, the model can then be implemented in just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d567586",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MLPScratch)\n",
    "def forward(self, X):\n",
    "    X = X.reshape((-1, self.num_inputs))\n",
    "    H = relu(torch.matmul(X, self.W1) + self.b1)\n",
    "    return torch.matmul(H, self.W2) + self.b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3cad13",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1768b",
   "metadata": {},
   "source": [
    "Luckily, training an MLP follows the same loop as softmax regression: we set up the model, data, and trainer, and then call the fit method to run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)\n",
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33204627",
   "metadata": {},
   "source": [
    "### Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfdf96",
   "metadata": {},
   "source": [
    "As expected, using high-level APIs allows us to implement MLPs in an even more compact way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215b6aa",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81666bd3",
   "metadata": {},
   "source": [
    "Here, the only change is adding two fully connected layers instead of one: a hidden layer followed by an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(), nn.LazyLinear(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db762342",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5081d32",
   "metadata": {},
   "source": [
    "This modular design lets us keep model architecture independent from other considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee69f6f",
   "metadata": {},
   "source": [
    "## Numerical Stability and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15695a7",
   "metadata": {},
   "source": [
    "So far, we have used parameter initialization without much discussion, but the choice of initialization is critical. It affects numerical stability, interacts with activation functions, and influences optimization speed. Poor initialization can lead to exploding or vanishing gradients, so this section explores these issues and provides practical heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5da46",
   "metadata": {},
   "source": [
    "### Vanishing and Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d23cbd",
   "metadata": {},
   "source": [
    "In deep networks, gradients are computed as products of many matrices. This can lead to numerical instability, where the gradients become either extremely large (exploding gradients) or extremely small (vanishing gradients). Such unstable gradients harm optimization by causing destructive parameter updates or preventing learning altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542fa67",
   "metadata": {},
   "source": [
    "#### Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821236df",
   "metadata": {},
   "source": [
    "A major cause of the vanishing gradient problem is the choice of activation function applied after each layer. Historically, the sigmoid function was widely used because it resembled biological thresholding behavior, neurons firing fully or not at all. However, this choice often leads to vanishing gradients, as we will now examine more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "y = torch.sigmoid(x)\n",
    "y.backward(torch.ones_like(x))\n",
    "\n",
    "d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],\n",
    "         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34649890",
   "metadata": {},
   "source": [
    "The sigmoidâ€™s gradient vanishes for both very large and very small inputs. When backpropagating through many layers, unless the inputs stay in a narrow middle range, the gradients tend to disappear, often cutting off learning at some depth. This issue long hindered deep network training. ReLUs, while less biologically realistic, avoid this instability and have therefore become the standard choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd16d9",
   "metadata": {},
   "source": [
    "#### Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e426e4",
   "metadata": {},
   "source": [
    "The opposite issue, exploding gradients, can be just as problematic. For example, multiplying many Gaussian random matrices by an initial matrix causes the result to grow uncontrollably when the variance is set to 1. If this occurs during network initialization, gradient descent cannot converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.normal(0, 1, size=(4, 4))\n",
    "print('a single matrix \\n',M)\n",
    "for i in range(100):\n",
    "    M = M @ torch.normal(0, 1, size=(4, 4))\n",
    "print('after multiplying 100 matrices\\n', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca63e4",
   "metadata": {},
   "source": [
    "### Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7aae40",
   "metadata": {},
   "source": [
    "Parameter initialization is crucial for stable training. Default initialization often uses random values from a normal distribution, but this can lead to exploding or vanishing variances across layers. Xavier initialization (Glorot and Bengio, 2010) addresses this by scaling weights based on both the number of input and output units, ensuring balanced variance during forward and backward propagation. It typically samples from a Gaussian or uniform distribution with variance $2 / (n_{in} + n_{out})$, making it a widely used and effective method in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74925ac6",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4390d",
   "metadata": {},
   "source": [
    "A good predictive model should generalize well by being simple and robust to input noise. Bishop (1995) showed that training with noise is equivalent to regularization, making models smoother and more resilient. Building on this, Srivastava et al. (2014) introduced dropout, which randomly zeros out neurons during training to prevent overfitting and break co-adaptation between units. Standard dropout keeps the expected output unbiased by rescaling activations, and it has become a widely used regularization method in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850880c",
   "metadata": {},
   "source": [
    "### Implementation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df949ecb",
   "metadata": {},
   "source": [
    "To implement dropout for a layer, we sample from a Bernoulli distribution: each unit is kept with probability $1 - p$ and dropped with probability $p$. Practically, this can be done by drawing uniform random numbers and retaining units whose sample exceeds $p$. In code, a `dropout_layer` function drops elements of input $X$ with probability `dropout` and rescales the survivors by dividing by `1 - dropout` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    if dropout == 1: return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape) > dropout).float()\n",
    "    return mask * X / (1.0 - dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99810c0d",
   "metadata": {},
   "source": [
    "We can try out the `dropout_layer` function on some sample inputs by applying dropout with probabilities of 0, 0.5, and 1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
    "print('dropout_p = 0:', dropout_layer(X, 0))\n",
    "print('dropout_p = 0.5:', dropout_layer(X, 0.5))\n",
    "print('dropout_p = 1:', dropout_layer(X, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833f7d1",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb2b42a",
   "metadata": {},
   "source": [
    "In the model below, dropout is applied to the outputs of each hidden layer after activation. The dropout probability can be set individually for each layer, with smaller values often used near the input. Importantly, dropout is enabled only during training, not during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6dca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutMLPScratch(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
    "                 dropout_1, dropout_2, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lin1 = nn.LazyLinear(num_hiddens_1)\n",
    "        self.lin2 = nn.LazyLinear(num_hiddens_2)\n",
    "        self.lin3 = nn.LazyLinear(num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\n",
    "        if self.training:\n",
    "            H1 = dropout_layer(H1, self.dropout_1)\n",
    "        H2 = self.relu(self.lin2(H1))\n",
    "        if self.training:\n",
    "            H2 = dropout_layer(H2, self.dropout_2)\n",
    "        return self.lin3(H2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3d558",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f170c8",
   "metadata": {},
   "source": [
    "The training process here follows the same procedure as that used for MLPs discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n",
    "           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}\n",
    "model = DropoutMLPScratch(**hparams)\n",
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80f0cb",
   "metadata": {},
   "source": [
    "### Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c17cea",
   "metadata": {},
   "source": [
    "Using high-level APIs, dropout is implemented by adding a Dropout layer after each fully connected layer, with the dropout probability as its sole parameter. During training, it randomly zeros out outputs of the previous layer, while in testing it passes data through unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutMLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
    "                 dropout_1, dropout_2, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(),\n",
    "            nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(),\n",
    "            nn.Dropout(dropout_2), nn.LazyLinear(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e599905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DropoutMLP(**hparams)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043c9e3",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1647c19",
   "metadata": {},
   "source": [
    "### Ex.1 \n",
    "\n",
    "Provide an example where the gradients vanish for the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7175c",
   "metadata": {},
   "source": [
    "### Ex.2\n",
    "Why is it a bad idea to insert a hidden layer with a single neuron? What could go wrong?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118791c4",
   "metadata": {},
   "source": [
    "### Ex.3\n",
    "Can we initialize all weight parameters in linear regression or in softmax regression to the same value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8dea62",
   "metadata": {},
   "source": [
    "### Ex.4 \n",
    "Why is dropout not typically used at test time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
