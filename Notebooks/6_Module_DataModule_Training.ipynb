{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8b886b",
   "metadata": {},
   "source": [
    "# Module / DataModule / Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc6e39",
   "metadata": {},
   "source": [
    "Inspired by open-source libraries such as PyTorch Lightning 70 , at a high level we wish to have three classes: \n",
    "- (i) Module contains models, losses, and optimization methods; \n",
    "- (ii) DataModule provides data loaders for training and validation; \n",
    "- (iii) both classes are combined using the Trainer class, which allows us to train models on a variety of hardware platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59100f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbc663",
   "metadata": {},
   "source": [
    "## Step 1: Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca361722",
   "metadata": {},
   "source": [
    "At the very least we need three methods. \n",
    "- The first, __init__, stores the learnable parameters, \n",
    "- the __training_step__ method accepts a data batch to return the loss value, \n",
    "- and finally, __configure_optimizers__ returns the optimization method, or a list of them, that is used to update the learnable parameters. \n",
    "\n",
    "Optionally we can define \n",
    "- __validation_step__ to report the evaluation measures. \n",
    "\n",
    "Sometimes we put the code for computing the output into a separate __forward__ method to\n",
    "make it more reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module, d2l.HyperParameters):  #@save\n",
    "    # You may notice that Module is a subclass of nn.Module, the base class of neural networks in PyTorch.\n",
    "    \"\"\"The base class of models.\"\"\"\n",
    "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
    "        super().__init__() # invokes the initializer (__init__ method) of a parent (or superclass) when you’re working with class inheritance.\n",
    "        self.save_hyperparameters()\n",
    "        self.board = d2l.ProgressBoard()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, X):\n",
    "        assert hasattr(self, 'net'), 'Neural network is defined'\n",
    "        return self.net(X)\n",
    "    \n",
    "    def plot(self, key, value, train):\n",
    "        \"\"\"Plot a point in animation.\"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
    "        self.board.xlabel = 'epoch'\n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / \\\n",
    "                self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batches / \\\n",
    "                self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / \\\n",
    "                self.plot_valid_per_epoch\n",
    "        self.board.draw(x, value.to(d2l.cpu()).detach().numpy(),\n",
    "                        ('train_' if train else 'val_') + key,\n",
    "                        every_n=int(n))\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d733880",
   "metadata": {},
   "source": [
    "## Step 2: Set Up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971e6ea",
   "metadata": {},
   "source": [
    "- Quite frequently the __init__ method is used to prepare the data. This includes downloading and preprocessing if needed. \n",
    "\n",
    "- The __train_dataloader__ returns the data loader for the training dataset. A data loader is a (Python) generator that yields a data batch each time it is used. This batch is then fed into the __training_step__ method of Module to compute the loss. \n",
    "\n",
    "- There is an optional __val_dataloader__ to return the validation dataset loader. It behaves in the same manner, except that it yields data batches for the validation_step method in Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(d2l.HyperParameters):  #@save\n",
    "    \"\"\"The base class of data.\"\"\"\n",
    "    def __init__(self, root='../data', num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27b83e",
   "metadata": {},
   "source": [
    "## Step 3: Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce0057",
   "metadata": {},
   "source": [
    "The __Trainer__ class trains the learnable parameters in the Module class with data specified in DataModule. \n",
    "\n",
    "- The key method is __fit__, which accepts two arguments: model, an instance of Module, and data, an instance of DataModule. \n",
    "\n",
    "- It then iterates over the entire dataset __max_epochs__ times to train the model. As before, we will defer the implementation of this method to later chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22764608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.HyperParameters):  #@save\n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "    def prepare_data(self, data):\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "    def prepare_model(self, model):\n",
    "        model.trainer = self\n",
    "        model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "    def fit(self, model, data):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "    def fit_epoch(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074fe9b",
   "metadata": {},
   "source": [
    "## Synthetic Data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790197fe",
   "metadata": {},
   "source": [
    "In the example in th enext Notebook (Linear regression), we will use a syntthetic dataset, created to illustrate the imlementation on the NN for Liner Regression. Here you can take a look at how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daf42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRegressionData(d2l.DataModule):  #@save\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=5000, num_val=5000, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\n",
    "        # -1 is a special value in NumPy that means: “automatically calculate this dimension \n",
    "        # based on the array’s total size.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f44809",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16441f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features:', data.X[0],'\\nlabel:', data.y[0])\n",
    "print('number of training examples:', data.num_train)\n",
    "print('number of validation examples:', data.num_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d35feb",
   "metadata": {},
   "source": [
    "Now create the trinloader to load the train set in batches to pass to the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train):\n",
    "    if train:\n",
    "        indices = list(range(0, self.num_train))\n",
    "        # The examples are read in random order\n",
    "        random.shuffle(indices)\n",
    "    else:\n",
    "        indices = list(range(self.num_train, self.num_train+self.num_val))\n",
    "    for i in range(0, len(indices), self.batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: i+self.batch_size])\n",
    "        # Pauses Execution and Returns a Value: When a generator function encounters a yield statement, \n",
    "        # it pauses its execution and returns the value specified by yield to the caller.\n",
    "        yield self.X[batch_indices], self.y[batch_indices] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)\n",
    "\n",
    "# check number of batches\n",
    "num_batches = sum(1 for _ in iter(data.train_dataloader()))\n",
    "print(\"Total batches:\", num_batches)\n",
    "#  NOTE:\n",
    "# iter(...) The iter() function is a built-in Python function that \n",
    "# returns an iterator from an iterable. In this case, it turns the \n",
    "# data loader (which is iterable) into an explicit iterator. \n",
    "# This allows you to manually retrieve elements using next().\n",
    "\n",
    "# next(...)\n",
    "# The next() function retrieves the next item from the iterator \n",
    "# — in this context, it gives you the first batch of data from the \n",
    "# data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55014bc",
   "metadata": {},
   "source": [
    "Rather than writing our own iterator, we can call the existing API in a framework to load data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                       shuffle=train)\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ef185",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46debeb",
   "metadata": {},
   "source": [
    "## Exercizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9669f",
   "metadata": {},
   "source": [
    "### Ex 1:\n",
    "\n",
    "Locate full implementations of the above classes that are saved in the D2L library. We strongly recommend that you look at the implementation in detail once you have gained some more familiarity with deep learning modeling.\n",
    "\n",
    "https://github.com/d2l-ai/d2l-en/tree/master/d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e866ea4",
   "metadata": {},
   "source": [
    "### Ex 2: Do the following:\n",
    "\n",
    "- print the shape of the input X and the label y generated in the example in cell 14\n",
    "- print the length for the train_dataloader object: what does this correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c15ef3",
   "metadata": {},
   "source": [
    "### Ex 3:\n",
    "\n",
    "The code below plots the data set that has been simulated before. Can you overlay the ground truth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de211732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(data.X[:, 0].detach().numpy(), data.y.detach().numpy(),\n",
    "            1, color='C0', alpha=0.5)\n",
    "plt.scatter(data.X[:, 1].detach().numpy(), data.y.detach().numpy(),\n",
    "            1, color='C1', alpha=0.5)\n",
    "# Hint: create and X array and the function y = 2*x + 4.2 and -3.4*x + 4.2 to plot the ground truth\n",
    "# ...\n",
    "# ...\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('x1 / x2')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Regression Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e38f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf1c2863",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74698741",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85b2335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10eb99a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(data.X[:, 0].detach().numpy(), \n",
    "#          2 * data.X[:, 0].detach().numpy() + 4.2, \n",
    "#          color='C0', linewidth=2, label=r'y = 2*x$_1$ + 4.2')\n",
    "# plt.plot(data.X[:, 1].detach().numpy(),\n",
    "#          -3.4 * data.X[:, 1].detach().numpy() + 4.2, \n",
    "#          color='C1', linewidth=2, label=r'y = -3.4*x$_2$ + 4.2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
