{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8b886b",
   "metadata": {},
   "source": [
    "# Getting familiar with torch.autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146f8c3",
   "metadata": {},
   "source": [
    "Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors.\n",
    "\n",
    "Training a NN happens in two steps:\n",
    "\n",
    "- Forward Propagation: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "- Backward Propagation: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58277c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a92026",
   "metadata": {},
   "source": [
    "### Vector to scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the x vector\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa87af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)  # The gradient is None by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the y = f(x) function returning a scalar\n",
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec4cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf649a",
   "metadata": {},
   "source": [
    "### Vector to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e7432",
   "metadata": {},
   "source": [
    "Because y is a vector, we must pass a gradient argument to backward(). \n",
    "\n",
    "We pass $v^·µÄ$ with the same length as y and has values 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "\n",
    "y = 3 * x**2\n",
    "print('y:', y)\n",
    "\n",
    "gradient_value = [1., 1.] # here is to show how to use the gradient argument in the backward function\n",
    "# it is not necessary to use this argument, but it can be useful in some cases, to scale the gradients (see example below)\n",
    "y.backward(torch.tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00342eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10.] # here is to show how to use the gradient argument\n",
    "y.backward(torch.tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dc1b2",
   "metadata": {},
   "source": [
    "One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29fa8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a): \n",
    "    b=a*2\n",
    "    while b.norm() < 1000: \n",
    "        b=b*2\n",
    "    if b.sum() > 0: \n",
    "        c=b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af00944",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "print('a:', a)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "print('a.grad:', a.grad)\n",
    "# check that gradient is f(a)/a as expected dince the function is f(a) = constant * a\n",
    "a.grad == f(a)/a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc1a36",
   "metadata": {},
   "source": [
    "## Exercizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71da275",
   "metadata": {},
   "source": [
    "### Ex 1 -- Practice with this topic, you can follow the step-by-step tutorial here\n",
    " \n",
    "[*The Gradient Argument in PyTorch‚Äôs `backward()` Function Explained by Examples*](https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe8802",
   "metadata": {},
   "source": [
    "### Ex 2 -- Let ùëì (ùë•) = sin(ùë•). Plot the graph of ùëì and of its derivative ùëì ‚Ä≤. Do not exploit the fact that ùëì ‚Ä≤ (ùë•) = cos(ùë•) but rather use automatic differentiation to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35b66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf3a762e",
   "metadata": {},
   "source": [
    "### Ex 5 -- Let ùëì (ùë•) = ((log $x^2$) ¬∑ sin ùë•) + $ùë•^{‚àí1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa2fb3",
   "metadata": {},
   "source": [
    "Write out a dependency graph tracing results from ùë• to ùëì (ùë•):\n",
    "We'll identify intermediate steps starting from x and building up to f(x)\n",
    "\n",
    "- Start from: x\n",
    "\n",
    "- Intermediate computations:\n",
    "    - $x^2$ \n",
    "        - log($x^2$)\n",
    "    - sin(x) \n",
    "        - log($x^2$) * sin(x)\n",
    "    - 1/x\n",
    "        - log($x^2$) * sin(x) + 1/x\n",
    "\n",
    "Now use the chain rule to comput the analytical derivative of thea function; also comppute the gradient using autigrad and compare the two solutions by plotting them and see if they overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57efb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe37a4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
