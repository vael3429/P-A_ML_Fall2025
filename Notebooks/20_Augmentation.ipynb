{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ee2832",
   "metadata": {},
   "source": [
    "# Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2247fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "img = d2l.Image.open('../images/cat1.png')\n",
    "d2l.plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206ce56",
   "metadata": {},
   "source": [
    "This function runs the image augmentation method augmultiple times on the input image imgand shows all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab51da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):\n",
    "    Y = [aug(img) for _ in range(num_rows * num_cols)]\n",
    "    d2l.show_images(Y, num_rows, num_cols, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a05548",
   "metadata": {},
   "source": [
    "### Cropping and flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718382e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.RandomHorizontalFlip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.RandomVerticalFlip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0997c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_aug = torchvision.transforms.RandomResizedCrop((200, 200), scale=(0.1, 1), ratio=(0.5, 2))\n",
    "apply(img, shape_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027846a",
   "metadata": {},
   "source": [
    "### Changing Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc70156",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_aug = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "apply(img, color_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba44f3",
   "metadata": {},
   "source": [
    "### Combining Multiple Image Augmentation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "augs = torchvision.transforms.Compose([torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])\n",
    "apply(img, augs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c0f97",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc81425",
   "metadata": {},
   "source": [
    "We will fine-tune a ResNet model on a small dataset, which was pretrained on the ImageNet dataset. \n",
    "\n",
    "This small dataset consists of thousands of images with and without hot dogs. We will use the fine-tuned model to recognize hot dogs from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60643a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip', 'fba480ffa8aa7e0febbb511d181409f899b9baa5')\n",
    "data_dir = d2l.download_extract('hotdog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989aaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))\n",
    "test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e0b8c",
   "metadata": {},
   "source": [
    "The first 8 positive examples and the last 8 negative images are shown below. As you can\n",
    "see, the images vary in size and aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a57da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotdogs = [train_imgs[i][0] for i in range(8)]\n",
    "not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]\n",
    "d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f665936",
   "metadata": {},
   "source": [
    "During training, we first crop a random area of random size and random aspect ratio from\n",
    "the image, and then scale this area to a 224√ó224 input image. \n",
    "\n",
    "During testing, we scale both\n",
    "the height and width of an image to 256 pixels, and then crop a central 224 √ó 224 area as\n",
    "input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the means and standard deviations of the three RGB channels to\n",
    "# standardize each channel\n",
    "normalize = torchvision.transforms.Normalize([0.485, 0.456, 0.406], # means for R,G,B channel\n",
    "                                             [0.229, 0.224, 0.225]) # stds for  R,G,B channel\n",
    "\n",
    "train_augs = torchvision.transforms.Compose([torchvision.transforms.RandomResizedCrop(224),\n",
    "                                            torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            normalize])\n",
    "test_augs = torchvision.transforms.Compose([torchvision.transforms.Resize([256, 256]),\n",
    "                                            torchvision.transforms.CenterCrop(224),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda96666",
   "metadata": {},
   "source": [
    "We use ResNet-18, which was pretrained on the ImageNet dataset, as the source model.\n",
    "\n",
    "The pretrained source model instance contains a number of feature layers and an output\n",
    "layer `fc`. The main purpose of this division is to facilitate the fine-tuning of model parameters of all layers but the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824b040",
   "metadata": {},
   "source": [
    "The member variable `fc` of source model is given below: As a fully connected layer, it transforms ResNet‚Äôs final global average pooling outputs (512 features) into\n",
    "1000 class outputs of the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091dc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5f5e0",
   "metadata": {},
   "source": [
    "We then construct a new neural network as\n",
    "the target model. It is defined in the same way as the pretrained source model except that\n",
    "its number of outputs in the final layer is set to the number of classes in the target dataset\n",
    "(rather than 1000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37750cb7",
   "metadata": {},
   "source": [
    "In the code below, the model parameters before the output layer of the target model instance `finetune_net` are initialized to model parameters of the corresponding layers from\n",
    "the source model. We can only use a small learning rate to fine-tune such pretrained parameters.\n",
    "\n",
    "In contrast, model parameters in the output layer are randomly initialized and generally require a larger learning rate to be learned from scratch. Letting the\n",
    "base learning rate be ùúÇ, a learning rate of 10ùúÇ will be used to iterate the model parameters\n",
    "in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3437f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_net = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2) # we want 2 output classes (hotdog vs non-hotdog)\n",
    "\n",
    "nn.init.xavier_uniform_(finetune_net.fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's borrow some functions from the Builders Guide notebook\n",
    "\n",
    "def gpu(i=0):  #@save\n",
    "    \"\"\"Get a GPU device.\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    \n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "def num_gpus():  #@save\n",
    "    if torch.backends.mps.is_available():\n",
    "        return 1  # Only 1 MPS GPU is available\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.cuda.device_count()\n",
    "    \n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    return [gpu(i) for i in range(num_gpus())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If `param_group=True`, the model parameters in the output layer will be\n",
    "# updated using a learning rate ten times greater\n",
    "def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5, param_group=True):\n",
    "    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_augs),\n",
    "                                            batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=test_augs),\n",
    "                                            batch_size=batch_size)\n",
    "    devices = try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "                     if name not in [\"fc.weight\",\"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                  lr=learning_rate, weight_decay=0.001)\n",
    "        \n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    \n",
    "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11385e77",
   "metadata": {},
   "source": [
    "#### ATT: this takes hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fine_tuning(finetune_net, 5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453dc2e",
   "metadata": {},
   "source": [
    "For comparison, we define an identical model, but initialize all of its model parameters to\n",
    "random values. \n",
    "\n",
    "Since the entire model needs to be trained from scratch, we can use a larger\n",
    "learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_net = torchvision.models.resnet18()\n",
    "scratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)\n",
    "train_fine_tuning(scratch_net, 5e-4, param_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344204b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
